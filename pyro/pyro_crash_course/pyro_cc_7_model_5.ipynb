{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro Crash Course ‚Äì Notebook 7: Learning Nonlinear Drift with Neural Nets\n",
    "\n",
    "This final notebook adds a learnable **nonlinear function** into our probabilistic model. We use an **artificial neural network (ANN)** to model a shared drift that affects all measurements, extending the expressive power of our calibration framework.\n",
    "\n",
    "---\n",
    "\n",
    "### üîô Quick Recap\n",
    "Notebook 6 allowed each sensor to be classified as faulty or functioning, introducing discrete latent variables and structured priors per class. \n",
    "\n",
    "Now we go a step further: we assume that all devices experience some unknown nonlinear drift, perhaps due to an environmental factor not captured in the model. We use a neural network to learn this drift directly from the data‚Äîwhile still performing joint inference over:\n",
    " - Per-device scale and offset (`alpha`)\n",
    " - Device failure mode (`is_faulty`)\n",
    " - Shared nonlinear trends (`ANN(T_true)`)\n",
    " \n",
    "---\n",
    "\n",
    "### üéØ Learning Goals\n",
    "\n",
    "- Add a neural network to a Pyro model using torch.nn and pyro.module\n",
    "- Treat ANN parameters as deterministic learnable components\n",
    "- Combine continuous, discrete, and functional unknowns in a single model\n",
    "- Interpret the learned ANN as a shared, nonlinear correction to the measured data\n",
    "- Train models that resemble real calibration setups with subtle unknown effects\n",
    "\n",
    "---\n",
    "\n",
    "### üõ† Modeling Idea\n",
    "\n",
    "We assume each measurement is generated as:\n",
    "\n",
    "$$\n",
    "T_{\\text{meas}} = \\alpha_0 + \\alpha_1 \\cdot T_{\\text{true}} + \\text{ANN}(T_{\\text{true}}) + \\varepsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha_0, \\alpha_1$ are **latent calibration parameters per device**\n",
    "- $\\text{ANN}(T_{\\text{true}})$ is a **shared nonlinear drift**\n",
    "- $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is i.i.d. measurement noise\n",
    "- Devices may be **faulty**, in which case their calibration parameters are drawn from broader priors\n",
    "\n",
    "We treat the ANN as a fixed function during inference, but let its parameters be **optimized via the ELBO**‚Äîjust like in classical machine learning.\n",
    "\n",
    "\n",
    "\n",
    "For this, do the following:\n",
    "   1. Imports and definitions  \n",
    "   2. Build model and guide  \n",
    "   3. Perform inference  \n",
    "   4. Interpretations and illustrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/atlasoptimization/tutorials/blob/master/pyro/pyro_crash_course/pyro_cc_7_model_5.ipynb\n",
    ")\n",
    "\n",
    "> ‚ö†Ô∏è You do *not* need to sign in with GitHub to run this notebook.  \n",
    "> Just click the Colab badge and start executing.\n",
    "\n",
    "üìé Input: `sensor_measurements.csv` (generated in `pyro_cc_1_hello_dataset`) \n",
    "\n",
    "üìå This is the **final notebook** of the pyro crash course!\n",
    "\n",
    "\n",
    "This notebook and series are created for educational purposes by  [Dr. Jemil Avers Butt, Atlas Optimization GmbH](https://www.atlasoptimization.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start of Notebook 7\n",
    "\n",
    "## 1. Imports and definitions\n",
    "\n",
    "No changes here. Just setup ‚Äî `pyro`, `torch`, CSV import, reshaping, and seed.\n",
    "\n",
    "\n",
    "üìå This is the final notebook. It **introduces neural nets** to our probabilistic models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-13 14:57:25--  https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12270 (12K) [text/plain]\n",
      "Saving to: ‚Äòsensor_measurements.csv.1‚Äô\n",
      "\n",
      "sensor_measurements 100%[===================>]  11.98K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-13 14:57:25 (48.7 MB/s) - ‚Äòsensor_measurements.csv.1‚Äô saved [12270/12270]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    1. Imports and definitions\n",
    "\"\"\"\n",
    "\n",
    "# i) Imports\n",
    "# If you run this in Colab, you might get an import error as pyro is not\n",
    "# installed by default. In that case, uncomment the following command.\n",
    "# !pip install pyro-ppl\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pyro.infer import config_enumerate\n",
    "\n",
    "\n",
    "# ii) Definitions\n",
    "!wget https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
    "measurement_df = pandas.read_csv(\"sensor_measurements.csv\")\n",
    "n_device = measurement_df[\"sensor_id\"].nunique()\n",
    "n_measure = measurement_df[\"time_step\"].nunique()\n",
    "\n",
    "# Read out T_true and T_meas: the true temperature and the measured temperature\n",
    "T_true = torch.tensor(measurement_df[\"T_true\"].values).reshape(n_device, n_measure)\n",
    "T_meas = torch.tensor(measurement_df[\"T_measured\"].values).reshape(n_device, n_measure)\n",
    "\n",
    "# Assume standard deviation\n",
    "sigma_T_meas = 0.3\n",
    "\n",
    "# Fix random seed\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Build model and guide\n",
    "\n",
    "## üîß Model Specification\n",
    "\n",
    "We extend the hierarchical model from Notebook 6 by including a shared **nonlinear drift** term modeled by a neural network. This shared component captures global trends across all devices and is trained jointly with the latent variables.\n",
    "\n",
    "Each sensor is still either:\n",
    "\n",
    "- ‚úÖ **Functioning normally**: Parameters drawn from `ùí©([0, 1], Œ£)`\n",
    "- ‚ùå **Faulty**: Parameters drawn from `ùí©([0, 1], Œ£_faulty)` with much larger uncertainty\n",
    "\n",
    "As before, each device has a discrete latent variable:\n",
    "\n",
    "$$\n",
    "\\text{is_faulty}_i \\sim \\text{Bernoulli}(p_\\text{faulty})\n",
    "$$\n",
    "\n",
    "The covariance of the calibration prior depends on faultiness:\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{device},i} = \\Sigma_{\\text{normal}} + \\text{is_faulty}_i \\cdot (\\Sigma_{\\text{faulty}} - \\Sigma_{\\text{normal}})\n",
    "$$\n",
    "\n",
    "The calibration parameters (offset and scale) are sampled per device:\n",
    "\n",
    "$$\n",
    "\\alpha_i \\sim \\mathcal{N}(\\mu_\\alpha, \\Sigma_{\\text{device},i})\n",
    "\\quad \\text{where} \\quad\n",
    "\\mu_\\alpha = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Shared Nonlinear Drift\n",
    "\n",
    "On top of this per-device affine structure, we introduce a shared **neural network drift** `ANN(T_true)` that adjusts the predicted value globally:\n",
    "\n",
    "$$\n",
    "T_{\\text{meas}}[i,j] \\sim \\mathcal{N} \\left(\n",
    "    \\alpha_{i,0} + \\alpha_{i,1} \\cdot T_{\\text{true}}[i,j] + \\text{ANN}(T_{\\text{true}}[i,j]),\n",
    "    \\sigma_\\text{Tmeas}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "The drift function `ANN` is implemented via a PyTorch neural network. Unlike the device-specific `Œ±`, which vary across sensors, the neural net learns a **global nonlinear correction** shared by all sensors. This allows Pyro to:\n",
    "\n",
    "- Capture smooth distortions in temperature behavior\n",
    "- Avoid hardcoding the structure of the drift\n",
    "- Automatically learn patterns not explainable by offsets or scale\n",
    "\n",
    "The drift function is fully differentiable and fits naturally into the SVI framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.1 Define the ANN\n",
    "\n",
    "We define a **small feedforward neural network** that learns a global nonlinear drift correction over all temperature measurements. It's used as an additive term in the model:\n",
    "\n",
    "$$\n",
    "T_{\\text{meas}} \\approx \\text{offset} + \\text{scale} \\cdot T_{\\text{true}} + \\text{ANN}(T_{\\text{true}})\n",
    "$$\n",
    "\n",
    "### üîß Architecture\n",
    "\n",
    "- **Input:** scalar `T_true[i,j] ‚àà ‚Ñù`  \n",
    "- **Output:** scalar drift adjustment for each `[i,j]`  \n",
    "- **Layers:**\n",
    "  - `Linear(1 ‚Üí 8)` + `Tanh`\n",
    "  - `Linear(8 ‚Üí 8)` + `Tanh`\n",
    "  - `Linear(8 ‚Üí 1)`\n",
    "\n",
    "### üîç Purpose\n",
    "\n",
    "- Captures **global nonlinear trends** shared across all sensors\n",
    "- Complements the **per-device linear calibration model**\n",
    "- Is treated as a deterministic function with trainable parameters, optimized via ELBO\n",
    "\n",
    "### ‚úÖ Notes\n",
    "\n",
    "- The network is built as a subclass of `torch.nn.Module`\n",
    "- Registered via `pyro.module(\"ann\", ann)` to ensure parameters are included in optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) Define the ANN)\n",
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize instance using init method from base class\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create linear and nonlinear transforms\n",
    "        self.lin_1 = torch.nn.Linear(1,8)\n",
    "        self.lin_2 = torch.nn.Linear(8,8)\n",
    "        self.lin_3 = torch.nn.Linear(8,1)\n",
    "        self.nonlinear = torch.nn.Tanh()\n",
    "    def forward(self, t):\n",
    "        # Define forward computation on the input data T_true.\n",
    "        t = t.reshape([-1, 1])\n",
    "        hidden_units_1 = self.nonlinear(self.lin_1(t))\n",
    "        hidden_units_2 = self.nonlinear(self.lin_2(hidden_units_1))\n",
    "        nonlinear_drift = self.lin_3(hidden_units_2)\n",
    "        \n",
    "        nonlinear_drift = nonlinear_drift.reshape([n_device, n_measure])\n",
    "        return nonlinear_drift\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Neural Networks in Pyro\n",
    "\n",
    "Neural networks can serve as powerful nonlinear function approximators inside probabilistic models. In Pyro, you can treat them as deterministic modules with learnable parameters ‚Äî useful for modeling **unknown systematics**, **sensor drift**, **nonlinear response functions**, and more.\n",
    "\n",
    "Suppose we model an observation $y$ as:\n",
    "\n",
    "$$\n",
    "y  \\approx \\text{ANN}(x) + \\varepsilon\n",
    "$$\n",
    "\n",
    "Here, the neural network learns a data-driven **nonlinear mapping** to map $x$ to $y$. We showcase below an example of how ANN's are built in pyro and then integrated into pyro models. Typically, this involves three steps:\n",
    "\n",
    "- Define a class ANN and the layers to be used (subclass torch.nn.Module and use torch.nn differentiable layers)\n",
    "- Define the ANN classes forward() method by chaining the layers (= forward pass of the ANN)\n",
    "- Invoke an instance of ANN and register it inside of the model via pyro.module() (= mark params for optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Minimal Pyro Pattern\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "# 1. Define ANN (standard torch.nn API works fine)\n",
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize instance using init method from base class\n",
    "        super().__init__()\n",
    "        \n",
    "        # transforms: linear, nonlinear\n",
    "        self.lin_1 = torch.nn.Linear(1,64)\n",
    "        self.lin_2 = torch.nn.Linear(64,1)\n",
    "        self.nonlinear = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape, then define how to chain the operations on input x\n",
    "        x = x.reshape([-1, 1])\n",
    "        hidden_units = self.nonlinear(self.lin_1(x))\n",
    "        nonlinear_drift = self.lin_2(hidden_units)\n",
    "        return nonlinear_drift\n",
    "\n",
    "# 2. Instantiate, then use as normal differentiable function\n",
    "ann = ANN()\n",
    "x_example = torch.linspace(0,1,100)\n",
    "y = ann(x_example)\n",
    "\n",
    "def model(x, y_obs=None):\n",
    "    pyro.module(\"ann\", ann)  # <- register all trainable params\n",
    "    # ... rest of model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define model and guide\n",
    "\n",
    "The subsequent code for model and guide are essentially identical to the code provided in the previous notebook. The only differences are marking the params inside of the ann for optimization (via `pyro.module(\"ann\", ann)`) and adding the neural network term to the mean (via `mean_obs = ... + ann(input_vars`)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Define the model\n",
    "\n",
    "# Define priors and fixed params\n",
    "mu_alpha = torch.tensor([0.0, 1.0]).expand(n_device, -1)\n",
    "Sigma_alpha = torch.tensor([[0.1**2,0], [0, 0.1**2]])\n",
    "p_faulty = 0.05\n",
    "Sigma_faulty = 100*Sigma_alpha\n",
    "\n",
    "# Now instantiate the neural net ann; set the params to double to match the data\n",
    "ann = ANN().double()\n",
    "\n",
    "# Build the model in pyro; just mix in ann() like any normal torch function\n",
    "@config_enumerate\n",
    "def model(input_vars = T_true, observations = None):\n",
    "    # Mark the parameters inside of the ann for optimization\n",
    "    pyro.module(\"ann\", ann)\n",
    "    \n",
    "    # Build reusable independence context device_plate and sample\n",
    "    device_plate = pyro.plate('device_plate', size=n_device, dim=-1)\n",
    "    with device_plate:\n",
    "        is_faulty = pyro.sample('is_faulty', pyro.distributions.Bernoulli(p_faulty))\n",
    "    is_faulty_tensor = is_faulty.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    # Build alpha_dist and sample\n",
    "    Sigma_device = Sigma_alpha + is_faulty_tensor * (Sigma_faulty - Sigma_alpha)  # shape: [n_device, 2, 2]\n",
    "    alpha_dist = pyro.distributions.MultivariateNormal(\n",
    "        loc=mu_alpha,                   # shape: [2, n_device, 2]\n",
    "        covariance_matrix=Sigma_device)     # shape: [2, n_device, 2, 2]\n",
    "    with device_plate:\n",
    "        alpha = pyro.sample(\"alpha\", alpha_dist)\n",
    "        \n",
    "    # Build observation dist and sample\n",
    "    mean_obs = alpha[:, 0].unsqueeze(1) + alpha[:, 1].unsqueeze(1) * T_true + ann(input_vars)\n",
    "    obs_dist = pyro.distributions.Normal(loc=mean_obs, scale=sigma_T_meas)\n",
    "    with pyro.plate('device_plate', dim=-2):\n",
    "        with pyro.plate('measure_plate', dim=-1):\n",
    "            pyro.sample(\"observations\", obs_dist, obs=observations)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Build the guide\n",
    "@config_enumerate\n",
    "def guide(input_vars = T_true, observations = None):\n",
    "    # Build plate and sample is_faulty\n",
    "    device_plate = pyro.plate('device_plate', size=n_device, dim=-1)\n",
    "    probs_faulty = pyro.param(\"probs_faulty\", 0.05 * torch.ones(n_device),\n",
    "                              constraint=pyro.distributions.constraints.unit_interval)\n",
    "    with device_plate:\n",
    "        is_faulty = pyro.sample(\"is_faulty\", pyro.distributions.Bernoulli(probs_faulty))\n",
    "    \n",
    "    # Per-device means and scales (shape [n_device, 2])\n",
    "    mu_alpha_post = pyro.param('mu_alpha_post', torch.tensor([0,1.0]).unsqueeze(0).expand([n_device,2]))\n",
    "    sigma_alpha_post = (pyro.param('sigma_alpha_post', 0.1 * (torch.eye(2).unsqueeze(0)).expand([n_device,2,2]),\n",
    "                              constraint=pyro.distributions.constraints.positive_definite) \n",
    "                        + 0.001 * torch.eye(2))\n",
    "    \n",
    "    # Sample alpha\n",
    "    with device_plate:\n",
    "        # Multivariate Gaussian for each device (allow for correlations)\n",
    "        alpha = pyro.sample('alpha', pyro.distributions.MultivariateNormal(loc = mu_alpha_post,\n",
    "                                        covariance_matrix = sigma_alpha_post))\n",
    "    return alpha, is_faulty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Tensor Shapes Table\n",
    "\n",
    "| Variable              | Shape                   | Description |\n",
    "|-----------------------|--------------------------|-------------|\n",
    "| `T_true`              | `[n_device, n_measure]`   | True temperature values (input) |\n",
    "| `T_meas`              | `[n_device, n_measure]`   | Observed noisy measurements |\n",
    "| `alpha`               | `[n_device, 2]`           | Offset and scale per device |\n",
    "| `mu_alpha_post`       | `[n_device, 2]`           | Posterior means of `alpha` |\n",
    "| `sigma_alpha_post`    | `[n_device, 2, 2]`        | Posterior covariances of `alpha` |\n",
    "| `is_faulty`           | `[n_device]`              | Binary latent variable for each device |\n",
    "| `Sigma_device`        | `[n_device, 2, 2]`        | Covariance matrix (depends on `is_faulty`) |\n",
    "| `posterior_faulty`    | `[n_samples, n_device]`   | Posterior samples of `is_faulty` |\n",
    "| `ann(T_true)`         | `[n_device, n_measure]`   | Global nonlinear drift (shared) |\n",
    "\n",
    "Purely structurally, the addition of the ANN changes very little in our model. We simply have one more function whose parameters need training. We illustrate the overall model below. You will be able to see the inclusion of multiple new parameters called `ann$$$lin__1.weight` and similar - they represent the inside of the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) Illustrate model and guide\n",
    "# Now the illustration of the model shows additional parameters representing the\n",
    "# biases and weights of the linear layers in the neural network. The guide looks\n",
    "# the same as before.\n",
    "graphical_model = pyro.render_model(model = model, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "graphical_guide = pyro.render_model(model = guide, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"336pt\" height=\"365pt\"\n",
       " viewBox=\"0.00 0.00 336.00 365.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 361)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-361 332,-361 332,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_device_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-8 8,-311.5 150,-311.5 150,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">device_plate</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_measure_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"16,-39 16,-114 142,-114 142,-39 16,-39\"/>\n",
       "<text text-anchor=\"middle\" x=\"94.5\" y=\"-46.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">measure_plate</text>\n",
       "</g>\n",
       "<!-- is_faulty -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>is_faulty</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"79\" cy=\"-285.5\" rx=\"41.6928\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-281.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">is_faulty</text>\n",
       "</g>\n",
       "<!-- alpha -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>alpha</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"79\" cy=\"-160\" rx=\"29.795\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-156.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha</text>\n",
       "</g>\n",
       "<!-- is_faulty&#45;&gt;alpha -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>is_faulty&#45;&gt;alpha</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M79,-267.2512C79,-246.5245 79,-212.4521 79,-188.2775\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"82.5001,-188.2708 79,-178.2708 75.5001,-188.2708 82.5001,-188.2708\"/>\n",
       "</g>\n",
       "<!-- observations -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>observations</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"79\" cy=\"-88\" rx=\"55.4913\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">observations</text>\n",
       "</g>\n",
       "<!-- alpha&#45;&gt;observations -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>alpha&#45;&gt;observations</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M79,-141.8314C79,-134.131 79,-124.9743 79,-116.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"82.5001,-116.4132 79,-106.4133 75.5001,-116.4133 82.5001,-116.4132\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-341.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">is_faulty ~ Bernoulli</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-326.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha ~ MultivariateNormal</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-311.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">observations ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-296.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ann$$$lin_1.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-281.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ann$$$lin_1.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-266.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ann$$$lin_2.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-251.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ann$$$lin_2.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-236.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ann$$$lin_3.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-221.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ann$$$lin_3.bias : Real()</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f5a6c5f2a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"681pt\" height=\"202pt\"\n",
       " viewBox=\"0.00 0.00 681.00 202.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 198)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-198 677,-198 677,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_device_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-8 8,-83 185,-83 185,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"142.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">device_plate</text>\n",
       "</g>\n",
       "<!-- probs_faulty -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>probs_faulty</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"86.5,-160 17.5,-160 17.5,-145 86.5,-145 86.5,-160\"/>\n",
       "<text text-anchor=\"middle\" x=\"52\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">probs_faulty</text>\n",
       "</g>\n",
       "<!-- is_faulty -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>is_faulty</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"58\" cy=\"-57\" rx=\"41.6928\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"58\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">is_faulty</text>\n",
       "</g>\n",
       "<!-- probs_faulty&#45;&gt;is_faulty -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>probs_faulty&#45;&gt;is_faulty</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M52.491,-144.6848C53.2966,-131.8629 54.9374,-105.7461 56.2198,-85.3343\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"59.7174,-85.4808 56.8515,-75.281 52.7312,-85.0418 59.7174,-85.4808\"/>\n",
       "</g>\n",
       "<!-- mu_alpha_post -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>mu_alpha_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"189,-160 105,-160 105,-145 189,-145 189,-160\"/>\n",
       "<text text-anchor=\"middle\" x=\"147\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mu_alpha_post</text>\n",
       "</g>\n",
       "<!-- alpha -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>alpha</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"147\" cy=\"-57\" rx=\"29.795\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"147\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha</text>\n",
       "</g>\n",
       "<!-- mu_alpha_post&#45;&gt;alpha -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>mu_alpha_post&#45;&gt;alpha</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M147,-144.6848C147,-131.8629 147,-105.7461 147,-85.3343\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.5001,-85.2809 147,-75.281 143.5001,-85.281 150.5001,-85.2809\"/>\n",
       "</g>\n",
       "<!-- sigma_alpha_post -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>sigma_alpha_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"306.5,-160 207.5,-160 207.5,-145 306.5,-145 306.5,-160\"/>\n",
       "<text text-anchor=\"middle\" x=\"257\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sigma_alpha_post</text>\n",
       "</g>\n",
       "<!-- sigma_alpha_post&#45;&gt;alpha -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>sigma_alpha_post&#45;&gt;alpha</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M247.9982,-144.6848C231.5867,-130.4367 196.2657,-99.7716 172.065,-78.7609\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.1732,-75.9563 164.3274,-72.0434 169.5841,-81.2422 174.1732,-75.9563\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"333\" y=\"-178.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">is_faulty ~ Bernoulli</text>\n",
       "<text text-anchor=\"start\" x=\"333\" y=\"-163.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha ~ MultivariateNormal</text>\n",
       "<text text-anchor=\"start\" x=\"333\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">probs_faulty : Interval(lower_bound=0.0, upper_bound=1.0)</text>\n",
       "<text text-anchor=\"start\" x=\"333\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mu_alpha_post : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"333\" y=\"-118.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sigma_alpha_post : PositiveDefinite()</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f5b48127400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphical_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) Record example outputs of model and guide prior to training\n",
    "n_model_samples = 10\n",
    "n_guide_samples = 1000  \n",
    "\n",
    "predictive = pyro.infer.Predictive\n",
    "prior_predictive_pretrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_pretrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_pretrain_faulty = predictive(guide, num_samples = n_guide_samples)()['is_faulty']\n",
    "posterior_predictive_pretrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Perform inference\n",
    "\n",
    "## üß† Inference with Discrete Latents and Neural Networks\n",
    "\n",
    "To train the model, we minimize the **Evidence Lower Bound (ELBO)** using stochastic variational inference:\n",
    "\n",
    "$$\n",
    "\\text{ELBO} = \\mathbb{E}_{q_\\phi(z)}[\\log p(x, z) - \\log q_\\phi(z)]\n",
    "$$\n",
    "\n",
    "Our model now includes:\n",
    "\n",
    "- Discrete latent variables (`is_faulty`)  \n",
    "- Continuous latent variables (`alpha`)  \n",
    "- Learnable parameters from a neural network (`ann`)  \n",
    "\n",
    "We use:\n",
    "\n",
    "- `TraceEnum_ELBO` to **enumerate** over the discrete latent variables\n",
    "- Monte Carlo sampling (`num_particles > 1`) for the continuous latents\n",
    "- Gradient-based learning for the neural net weights\n",
    "\n",
    "This enables us to **jointly learn**:\n",
    "- The per-device calibration parameters  \n",
    "- The probability of each device being faulty  \n",
    "- A shared **nonlinear drift function** across all devices  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 ; loss : 53707.7534308528\n",
      "epoch: 50 ; loss : 29297.56991749781\n",
      "epoch: 100 ; loss : 19321.89543800371\n",
      "epoch: 150 ; loss : 13025.876104981562\n",
      "epoch: 200 ; loss : 10747.238108977483\n",
      "epoch: 250 ; loss : 6785.122517551263\n",
      "epoch: 300 ; loss : 7692.150914395468\n",
      "epoch: 350 ; loss : 5003.652629845844\n",
      "epoch: 400 ; loss : 4657.5006386023515\n",
      "epoch: 450 ; loss : 4115.925911569804\n",
      "epoch: 500 ; loss : 3099.9811012303167\n",
      "epoch: 550 ; loss : 4025.54692285314\n",
      "epoch: 600 ; loss : 3537.755214095899\n",
      "epoch: 650 ; loss : 2999.8426709492965\n",
      "epoch: 700 ; loss : 3389.9496346707338\n",
      "epoch: 750 ; loss : 2402.21116471476\n",
      "epoch: 800 ; loss : 2253.8049590362116\n",
      "epoch: 850 ; loss : 2825.2036658646384\n",
      "epoch: 900 ; loss : 2377.1419803307413\n",
      "epoch: 950 ; loss : 2421.2894027985976\n",
      "epoch: 1000 ; loss : 1862.4557146664877\n",
      "epoch: 1050 ; loss : 1824.4013392658358\n",
      "epoch: 1100 ; loss : 1549.0122874880526\n",
      "epoch: 1150 ; loss : 2529.029961059539\n",
      "epoch: 1200 ; loss : 1723.998974197058\n",
      "epoch: 1250 ; loss : 1490.5591911995941\n",
      "epoch: 1300 ; loss : 1681.5351978101737\n",
      "epoch: 1350 ; loss : 1217.671841288989\n",
      "epoch: 1400 ; loss : 1695.5413192628266\n",
      "epoch: 1450 ; loss : 1803.4715730112816\n",
      "epoch: 1500 ; loss : 1401.173348760905\n",
      "epoch: 1550 ; loss : 2071.6345311583495\n",
      "epoch: 1600 ; loss : 1305.883222207019\n",
      "epoch: 1650 ; loss : 1250.418091291167\n",
      "epoch: 1700 ; loss : 1397.3525427232657\n",
      "epoch: 1750 ; loss : 1390.7463845580285\n",
      "epoch: 1800 ; loss : 1372.562484102673\n",
      "epoch: 1850 ; loss : 1193.6464249267835\n",
      "epoch: 1900 ; loss : 1038.7397701149773\n",
      "epoch: 1950 ; loss : 1038.5820500063157\n",
      "epoch: 2000 ; loss : 1384.2043086855522\n",
      "epoch: 2050 ; loss : 1498.677360689902\n",
      "epoch: 2100 ; loss : 1370.3133907488298\n",
      "epoch: 2150 ; loss : 1088.245581262684\n",
      "epoch: 2200 ; loss : 1179.013770041832\n",
      "epoch: 2250 ; loss : 1141.5105697912759\n",
      "epoch: 2300 ; loss : 913.2658119353886\n",
      "epoch: 2350 ; loss : 961.2753214854419\n",
      "epoch: 2400 ; loss : 905.633256845049\n",
      "epoch: 2450 ; loss : 1215.2709597687679\n",
      "epoch: 2500 ; loss : 938.3573820093784\n",
      "epoch: 2550 ; loss : 1092.0389678608544\n",
      "epoch: 2600 ; loss : 833.1291855834023\n",
      "epoch: 2650 ; loss : 737.2770967004872\n",
      "epoch: 2700 ; loss : 974.2746070910438\n",
      "epoch: 2750 ; loss : 1348.5613482809383\n",
      "epoch: 2800 ; loss : 1005.7151351708999\n",
      "epoch: 2850 ; loss : 749.5921954218467\n",
      "epoch: 2900 ; loss : 959.9633889032194\n",
      "epoch: 2950 ; loss : 747.8790406386031\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    3. Perform inference\n",
    "\"\"\"\n",
    "\n",
    "# i) Set up inference\n",
    "adam = pyro.optim.Adam({\"lr\": 0.01})\n",
    "elbo = pyro.infer.TraceEnum_ELBO(num_particles = 10,\n",
    "                                 max_plate_nesting = 2)\n",
    "svi = pyro.infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "\n",
    "# ii) Perform svi\n",
    "\n",
    "data = (T_true, T_meas)\n",
    "loss_sequence = []\n",
    "for step in range(3000):\n",
    "    loss = svi.step(*data)\n",
    "    loss_sequence.append(loss)\n",
    "    if step %50 == 0:\n",
    "        print(f'epoch: {step} ; loss : {loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå Learning the neural drift is challenging:  \n",
    "Increasing `num_particles` improves accuracy but may get stuck in local minima. Instead, we lower `num_particles` and train longer to encourage **model space exploration**. As a downside, running this cell might take 15 mins! This is the right point in time to already skip ahead with reading the summary and condensing the whole of the crash course for yourself.\n",
    "\n",
    "Note that training not converging may not always be due to the problem just being too hard - sometimes some subtle bugs can worm their way into the code. You can see some listed below. Especially notice that not calling `pyro.module()` to register the parameters inside of the ANN results in no training being done at all on the ANN parameters. This might not even be recognizable in the loss. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Troubleshooting Checklist\n",
    "\n",
    "| Symptom                      | Likely Cause                        | Fix                                                          |\n",
    "| ---------------------------- | ----------------------------------- | ------------------------------------------------------------ |\n",
    "| Parameters don't change      | `pyro.module()` is missing          | Add `pyro.module(\"ann\", ann)` inside `model()`               |\n",
    "| Gradients are `None` or zero | Model is disconnected from data     | Ensure `ann(x)` influences `obs`                             |\n",
    "| \"Parameter already exists\"   | Re-running cells in Jupyter         | Use `pyro.clear_param_store()`                               |\n",
    "| Dtype mismatch errors        | Mixed float32 / float64 tensors     | Use `torch.set_default_dtype(torch.float64)` and `.double()` |\n",
    "| Learning is unstable or flat | Poor init, too deep network, low LR | Try smaller network, warm-up, check signal-to-noise          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the shapes inside of the model : \n",
      "      Trace Shapes:          \n",
      "      Param Sites:          \n",
      "ann$$$lin_1.weight     8 1  \n",
      "  ann$$$lin_1.bias       8  \n",
      "ann$$$lin_2.weight     8 8  \n",
      "  ann$$$lin_2.bias       8  \n",
      "ann$$$lin_3.weight     1 8  \n",
      "  ann$$$lin_3.bias       1  \n",
      "     Sample Sites:          \n",
      " device_plate dist       |  \n",
      "             value     5 |  \n",
      "    is_faulty dist     5 |  \n",
      "             value     5 |  \n",
      "        alpha dist     5 | 2\n",
      "             value     5 | 2\n",
      " observations dist 5 100 |  \n",
      "             value 5 100 |  \n",
      "These are the shapes inside of the guide : \n",
      "     Trace Shapes:        \n",
      "     Param Sites:        \n",
      "     probs_faulty     5  \n",
      "    mu_alpha_post   5 2  \n",
      " sigma_alpha_post 5 2 2  \n",
      "    Sample Sites:        \n",
      "device_plate dist     |  \n",
      "            value   5 |  \n",
      "   is_faulty dist   5 |  \n",
      "            value   5 |  \n",
      "       alpha dist   5 | 2\n",
      "            value   5 | 2\n",
      "Param : ann$$$lin_1.weight; Value : Parameter containing:\n",
      "tensor([[-0.0478],\n",
      "        [-0.1240],\n",
      "        [-0.2142],\n",
      "        [-0.2522],\n",
      "        [-0.1271],\n",
      "        [ 0.2934],\n",
      "        [ 0.0906],\n",
      "        [ 0.2562]], dtype=torch.float64, requires_grad=True)\n",
      "Param : ann$$$lin_1.bias; Value : Parameter containing:\n",
      "tensor([ 0.3954, -1.1512,  2.7245,  4.4390, -0.0911, -3.1969, -1.1168, -4.6203],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Param : ann$$$lin_2.weight; Value : Parameter containing:\n",
      "tensor([[-0.0113, -0.0708, -0.5159, -0.3966, -0.1760,  0.8390, -0.0572,  0.5187],\n",
      "        [-0.2318,  0.0154,  0.2598, -0.4791, -0.1961,  0.4367,  0.1407,  0.3793],\n",
      "        [-0.8766, -0.3313, -1.4382, -2.1097, -0.6125,  2.3377,  1.5181,  1.2387],\n",
      "        [-0.2548, -0.0334, -0.9558, -1.5223, -0.1415,  0.8245,  0.8528,  1.7464],\n",
      "        [ 0.2418,  0.1498,  0.4418,  0.6572,  0.3065, -0.7162, -0.1890, -0.4554],\n",
      "        [ 0.0165, -0.2842, -0.4185, -0.3909,  0.1499, -0.0105, -0.1242,  0.2382],\n",
      "        [ 0.0513, -0.3959,  0.4063,  0.9496, -0.0358,  0.8455, -0.4063, -1.1108],\n",
      "        [ 0.4243,  0.2503,  1.0375,  1.5053,  0.2957, -1.3001, -0.4244, -0.9992]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Param : ann$$$lin_2.bias; Value : Parameter containing:\n",
      "tensor([-0.0091,  0.2730,  0.7004, -0.4697, -0.0885, -0.0458,  0.1846, -0.4039],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Param : ann$$$lin_3.weight; Value : Parameter containing:\n",
      "tensor([[-0.1108, -0.0700,  0.4028, -0.7326,  0.1964,  0.0167, -0.9544, -0.2678]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Param : ann$$$lin_3.bias; Value : Parameter containing:\n",
      "tensor([0.1440], dtype=torch.float64, requires_grad=True)\n",
      "Param : probs_faulty; Value : tensor([1.0000, 0.0052, 0.0143, 0.0157, 0.9982], grad_fn=<ClampBackward1>)\n",
      "Param : mu_alpha_post; Value : tensor([[ 4.3681,  0.9348],\n",
      "        [-0.0858,  1.0627],\n",
      "        [-0.1374,  1.1010],\n",
      "        [-0.1244,  0.9711],\n",
      "        [-0.6370,  1.0236]], requires_grad=True)\n",
      "Param : sigma_alpha_post; Value : tensor([[[ 0.0385, -0.0023],\n",
      "         [-0.0023,  0.0003]],\n",
      "\n",
      "        [[ 0.0143, -0.0014],\n",
      "         [-0.0014,  0.0003]],\n",
      "\n",
      "        [[ 0.0153, -0.0008],\n",
      "         [-0.0008,  0.0002]],\n",
      "\n",
      "        [[ 0.0153, -0.0009],\n",
      "         [-0.0009,  0.0002]],\n",
      "\n",
      "        [[ 0.0337, -0.0008],\n",
      "         [-0.0008,  0.0002]]], grad_fn=<UnsafeViewBackward0>)\n",
      "is_faulty ground truth : [1, 0, 0, 0, 0]\n",
      "Probabilities of being faulty pre-training : tensor([0.0450, 0.0480, 0.0570, 0.0500, 0.0470]) \n",
      "Probabilities of being faulty post-training : tensor([1.0000, 0.0050, 0.0130, 0.0150, 1.0000]) \n"
     ]
    }
   ],
   "source": [
    "# iii) Record example outputs of model and guide after training\n",
    "prior_predictive_posttrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_posttrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_posttrain_faulty = predictive(guide, num_samples = n_guide_samples)()['is_faulty']\n",
    "posterior_predictive_posttrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n",
    "\n",
    "# iv) Additional investigations\n",
    "model_trace = pyro.poutine.trace(model).get_trace(T_true, observations = T_meas)\n",
    "guide_trace = pyro.poutine.trace(guide).get_trace(T_true, observations = T_meas)\n",
    "print('These are the shapes inside of the model : \\n {}'.format(model_trace.format_shapes()))\n",
    "print('These are the shapes inside of the guide : \\n {}'.format(guide_trace.format_shapes()))\n",
    "\n",
    "# The parameters of the posterior are again stored in pyro's param_store\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print('Param : {}; Value : {}'.format(name, value))\n",
    "\n",
    "# Estimate the posterior probabilities of each device being faulty. \n",
    "faulty_pretrain = torch.mean(posterior_pretrain_faulty,dim=0)\n",
    "faulty_posttrain = torch.mean(posterior_posttrain_faulty,dim=0)\n",
    "print('is_faulty ground truth : [1, 0, 0, 0, 0]')\n",
    "print('Probabilities of being faulty pre-training : {} \\n'\n",
    "      'Probabilities of being faulty post-training : {} '\n",
    "      .format(faulty_pretrain, faulty_posttrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Illustrations and interpretations\n",
    "\n",
    "We visualize:\n",
    "\n",
    "- The loss decrease during training\n",
    "- The estimated nonlinear drift function\n",
    "- Raw data alongside generated model outputs\n",
    "\n",
    "This provides immediate insight into how well the model fits each sensor and what the ANN contributes towards modelling the sensor data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    4. Interpretations and illustrations\n",
    "\"\"\"\n",
    "\n",
    "# i) Plot and print ELBO loss\n",
    "plt.figure(1, dpi = 300)\n",
    "plt.plot(loss_sequence)\n",
    "plt.yscale(\"log\")\n",
    "plt.title('ELBO loss during training (log scale)')\n",
    "plt.xlabel('Epoch nr')\n",
    "plt.ylabel('value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Showcase the nonlinear drift\n",
    "\n",
    "# SVI adjusted the parameters in the neural net such that the ELBO was reduced.\n",
    "plt.figure(2, dpi = 300)\n",
    "plt.plot(ann(T_true).detach().T)\n",
    "plt.title('Learned drift function')\n",
    "plt.xlabel('T_true')\n",
    "plt.ylabel('Drift value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iii) Compare model output and data\n",
    "\n",
    "# Create the figure and 2x5 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=False, sharey=False)\n",
    "# Global y-axis limits\n",
    "y_min = T_meas.min()\n",
    "y_max = T_meas.max()\n",
    "\n",
    "# FIRST ROW\n",
    "# First plot: measurement data\n",
    "for i in range(n_device):\n",
    "    axes[0,0].scatter(T_true[i,:], T_meas[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,0].set_title(\"Measurement data\")\n",
    "axes[0,0].set_xlabel(\"T_true\")\n",
    "axes[0,0].set_ylabel(\"T_meas\")\n",
    "axes[0,0].set_ylim(y_min, y_max)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Second plot: data produced by model pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,1].scatter(T_true[i,:], prior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,1].set_title(\"Data from model pre-training\")\n",
    "axes[0,1].set_xlabel(\"T_true\")\n",
    "axes[0,1].set_ylabel(\"T_meas\")\n",
    "axes[0,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,2].scatter(T_true[i,:], posterior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,2].set_title(\"Data from posterior predictive pre-training\")\n",
    "axes[0,2].set_xlabel(\"T_true\")\n",
    "axes[0,2].set_ylabel(\"T_meas\")\n",
    "axes[0,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide pre-training\n",
    "axes[0,3].hist2d(posterior_pretrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_pretrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[0,3].set_title(\"2D Histogram of parameters pre-train\")\n",
    "axes[0,3].set_xlabel(\"alpha_0\")\n",
    "axes[0,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "# SECOND ROW\n",
    "# Second plot: data produced by model post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,1].scatter(T_true[i,:], prior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,1].set_title(\"Data from model post-training\")\n",
    "axes[1,1].set_xlabel(\"T_true\")\n",
    "axes[1,1].set_ylabel(\"T_meas\")\n",
    "axes[1,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,2].scatter(T_true[i,:], posterior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,2].set_title(\"Data from posterior predictive post-training\")\n",
    "axes[1,2].set_xlabel(\"T_true\")\n",
    "axes[1,2].set_ylabel(\"T_meas\")\n",
    "axes[1,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide post-training\n",
    "axes[1,3].hist2d(posterior_posttrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_posttrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[1,3].set_title(\"2D Histogram of parameters post-train\")\n",
    "axes[1,3].set_xlabel(\"alpha_0\")\n",
    "axes[1,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßæ Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "‚úÖ Built a full probabilistic model with **continuous, discrete, and neural components**  \n",
    "‚úÖ Introduced a **shared nonlinear drift**, modeled by a neural network (`ann`)  \n",
    "‚úÖ Combined this with **per-device latent calibration parameters** and **outlier detection**  \n",
    "‚úÖ Trained all components using **stochastic variational inference** and `TraceEnum_ELBO`  \n",
    "‚úÖ Visualized how the model recovers **nonlinear effects and faulty devices** from data\n",
    "\n",
    "\n",
    "üìå This concludes our crash course in Pyro!\n",
    "\n",
    "With tools like:\n",
    "- Latent variable modeling (both discrete and continuous)\n",
    "- Hierarchical priors and structured plates\n",
    "- Inference over neural networks\n",
    "\n",
    "you‚Äôre now ready to tackle **real-world probabilistic modeling** problems ‚Äî from calibration to forecasting to anomaly detection.\n",
    "\n",
    "üéâ Thanks for following along!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
