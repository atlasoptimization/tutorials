{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro Crash Course ‚Äì Notebook 5: Hierarchical latent variables\n",
    "\n",
    "We now go fully Bayesian and hierarchical: This notebook introduces **latent random variables for each device** into our sensor calibration model. This enables inference with **posterior mean and covariance** different for each sensor thereby introducing multimodality into the inferred latent posteriors.\n",
    "\n",
    "### üîô Quick Recap\n",
    "\n",
    "In *Notebook 4*, we used one global offset/scale pair for all devices.  \n",
    "Here, we expand that to per-device latent parameters:\n",
    "\n",
    "$$\n",
    "T_{\\text{meas}}[i, j] = \\alpha_0^{(i)} + \\alpha_1^{(i)} \\cdot T_{\\text{true}}[i, j] + \\text{noise}\n",
    "$$\n",
    "\n",
    "Each pair $(\\alpha_0^{(i)}, \\alpha_1^{(i)})$ is drawn from a shared prior:\n",
    "\n",
    "$$\n",
    "\\alpha^{(i)} \\sim \\mathcal{N}\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, 0.1^2 I\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Learning Goals\n",
    "\n",
    "- Implement a **hierarchical Bayesian model** with sensor-specific latents  \n",
    "- Learn **per-sensor posterior distributions** (mean + covariance)  \n",
    "- Use **multivariate normal** as a variational family with full covariance  \n",
    "- Detect **model misspecification or sensor anomalies** via the posterior\n",
    "\n",
    "---\n",
    "\n",
    "For this, do the following:\n",
    "   1. Imports and definitions  \n",
    "   2. Build model and guide  \n",
    "   3. Perform inference  \n",
    "   4. Interpretations and illustrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/atlasoptimization/tutorials/blob/master/pyro/pyro_crash_course/pyro_cc_5_model_3.ipynb\n",
    ")\n",
    "\n",
    "> ‚ö†Ô∏è You do *not* need to sign in with GitHub to run this notebook.  \n",
    "> Just click the Colab badge and start executing.\n",
    "\n",
    "üìé Input: `sensor_measurements.csv` (generated in `pyro_cc_1_hello_dataset`) \n",
    "\n",
    "üìå Next: `pyro_cc_6_model_4` introduces a discrete distribution that allows classifying a a faulty device\n",
    "\n",
    "\n",
    "This notebook and series are created for educational purposes by  [Dr. Jemil Avers Butt, Atlas Optimization GmbH](https://www.atlasoptimization.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start of Notebook 5\n",
    "\n",
    "## 1. Imports and definitions\n",
    "\n",
    "No changes here. Just setup ‚Äî `pyro`, `torch`, CSV import, reshaping, and seed.\n",
    "\n",
    "\n",
    "üìå This is the first notebook that **introduces per-device latent variables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-09 00:22:53--  https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12270 (12K) [text/plain]\n",
      "Saving to: ‚Äòsensor_measurements.csv.2‚Äô\n",
      "\n",
      "sensor_measurements 100%[===================>]  11.98K  3.93KB/s    in 3.0s    \n",
      "\n",
      "2025-05-09 00:23:07 (3.93 KB/s) - ‚Äòsensor_measurements.csv.2‚Äô saved [12270/12270]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    1. Imports and definitions\n",
    "\"\"\"\n",
    "\n",
    "# i) Imports\n",
    "# If you run this in Colab, you might get an import error as pyro is not\n",
    "# installed by default. In that case, uncomment the following command.\n",
    "# !pip install pyro-ppl\n",
    "import pyro\n",
    "import torch\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ii) Definitions\n",
    "!wget https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
    "measurement_df = pandas.read_csv(\"sensor_measurements.csv\")\n",
    "n_device = measurement_df[\"sensor_id\"].nunique()\n",
    "n_measure = measurement_df[\"time_step\"].nunique()\n",
    "\n",
    "# Read out T_true and T_meas: the true temperature and the measured temperature\n",
    "T_true = torch.tensor(measurement_df[\"T_true\"].values).reshape(n_device, n_measure)\n",
    "T_meas = torch.tensor(measurement_df[\"T_measured\"].values).reshape(n_device, n_measure)\n",
    "\n",
    "# Assume standard deviation\n",
    "sigma_T_meas = 0.3\n",
    "\n",
    "# Fix random seed\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Build Model and Guide\n",
    "\n",
    "### üîß The Model\n",
    "\n",
    "We now treat each sensor's offset and scale as latent variables:  \n",
    "Their values are *sampled inside* a Pyro plate over devices.\n",
    "\n",
    "- The prior: $\\alpha_i \\sim \\mathcal{N}([0, 1], 0.1^2 I)$  \n",
    "- The likelihood:  \n",
    "  $T_{\\text{meas}}[i,j] \\sim \\mathcal{N}(\\alpha^{(i)}_0 + \\alpha^{(i)}_1 T_{\\text{true}}[i,j], \\sigma)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2. Build model and guide\n",
    "\"\"\"\n",
    "\n",
    "# i) Define the model\n",
    "# Define priors\n",
    "mu_alpha = torch.tensor([0.0, 1.0])\n",
    "Sigma_alpha = torch.tensor([[0.1**2,0], [0, 0.1**2]])\n",
    "\n",
    "# Build the model in pyro\n",
    "def model(input_vars = T_true, observations = None):\n",
    "    # Define latent dist, then sample latents alpha inside of device_plate\n",
    "    alpha_dist = pyro.distributions.MultivariateNormal(loc = mu_alpha,\n",
    "                                                covariance_matrix = Sigma_alpha)\n",
    "    \n",
    "    # Independence in first dim of alpha_dist, sample n_device times\n",
    "    with pyro.plate('device_plate', size = n_device, dim = -1):\n",
    "        # We sample alpha from a multivariate normal prior\n",
    "        alpha = pyro.sample('alpha', alpha_dist)\n",
    "    \n",
    "    # Build the observation distribution\n",
    "    # This requires a bit of reshaping to achieve broadcastable shapes.\n",
    "    mean_obs = (alpha[:,0].unsqueeze(1)) + (alpha[:,1].unsqueeze(1))*T_true\n",
    "    obs_dist = pyro.distributions.Normal(loc = mean_obs, scale = sigma_T_meas)\n",
    "    \n",
    "    # Sample from this distribution and declare the samples independent in the\n",
    "    # first two dims. \n",
    "    with pyro.plate('device_plate', dim = -2):\n",
    "        with pyro.plate('measure_plate', dim = -1):\n",
    "            obs = pyro.sample('observations', obs_dist, obs = observations)\n",
    "    \n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Key Implementation Notes\n",
    "\n",
    "- `pyro.sample('alpha', ...)` occurs **within the device plate**  \n",
    "  ‚áí one draw per sensor, shape `[n_device, 2]`\n",
    "\n",
    "- Reshaping is used to broadcast the model linearly:  \n",
    "  `mean_obs = alpha[:,0].unsqueeze(1) + alpha[:,1].unsqueeze(1) * T_true`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† The Guide\n",
    "\n",
    "We approximate each latent pair $\\alpha_i = (\\alpha_0^{(i)}, \\alpha_1^{(i)})$  \n",
    "with its **own** variational multivariate normal distribution:\n",
    "\n",
    "$$\n",
    "q(\\alpha^{(i)}) = \\mathcal{N}(\\mu_i, \\Sigma_i)\n",
    "$$\n",
    "\n",
    "- We store $\\mu_i$ and $\\Sigma_i$ using `pyro.param`\n",
    "- Each $\\Sigma_i$ is a full covariance matrix (2x2), per device\n",
    "- Constraint: `positive_definite` ensures valid covariance\n",
    "- A diagonal jitter term `+ 0.001 * I` improves numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Build the guide\n",
    "\n",
    "# Build the guide\n",
    "def guide(input_vars = T_true, observations = None):\n",
    "    # Per-device means and scales (shape [n_device, 2])\n",
    "    mu_alpha_post = pyro.param('mu_alpha_post', torch.tensor([0,1.0]).unsqueeze(0).expand([n_device,2]))\n",
    "    sigma_alpha_post = (pyro.param('sigma_alpha_post', 0.1 * (torch.eye(2).unsqueeze(0)).expand([n_device,2,2]),\n",
    "                             constraint=pyro.distributions.constraints.positive_definite) \n",
    "                        + 0.001 * torch.eye(2))\n",
    "    # We add 1e-3 on the diagonal of the covariance matrix to avoid numerical issues\n",
    "    # related to positive definiteness tests.\n",
    "\n",
    "    with pyro.plate('device_plate', size=n_device, dim=-1):\n",
    "        # Multivariate Gaussian for each device (allow for correlations)\n",
    "        alpha = pyro.sample('alpha', pyro.distributions.MultivariateNormal(loc = mu_alpha_post,\n",
    "                                        covariance_matrix = sigma_alpha_post))\n",
    "\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Visualizing the Model Structure\n",
    "\n",
    "We use `pyro.render_model` to visualize both the **generative model** and the **guide**:\n",
    "\n",
    "- The model shows `alpha` **inside the device plate** as a latent variable  \n",
    "- The guide shows `alpha` as sampled from learned `mu` and `Sigma`, also plate-scoped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) Illustrate model and guide\n",
    "graphical_model = pyro.render_model(model = model, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "graphical_guide = pyro.render_model(model = guide, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"336pt\" height=\"203pt\"\n",
       " viewBox=\"0.00 0.00 336.00 203.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 199)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-199 332,-199 332,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_device_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-8 8,-187 150,-187 150,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">device_plate</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_measure_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"16,-39 16,-114 142,-114 142,-39 16,-39\"/>\n",
       "<text text-anchor=\"middle\" x=\"94.5\" y=\"-46.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">measure_plate</text>\n",
       "</g>\n",
       "<!-- alpha -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>alpha</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"79\" cy=\"-161\" rx=\"29.795\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-157.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha</text>\n",
       "</g>\n",
       "<!-- observations -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>observations</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"79\" cy=\"-88\" rx=\"55.4913\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">observations</text>\n",
       "</g>\n",
       "<!-- alpha&#45;&gt;observations -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>alpha&#45;&gt;observations</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M79,-142.9551C79,-134.8828 79,-125.1764 79,-116.1817\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"82.5001,-116.0903 79,-106.0904 75.5001,-116.0904 82.5001,-116.0903\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-164.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha ~ MultivariateNormal</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-149.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">observations ~ Normal</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fe034dc8040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"450pt\" height=\"172pt\"\n",
       " viewBox=\"0.00 0.00 450.00 172.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 168)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-168 446,-168 446,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_device_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"62.5,-8 62.5,-83 147.5,-83 147.5,-8 62.5,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">device_plate</text>\n",
       "</g>\n",
       "<!-- sigma_alpha_post -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>sigma_alpha_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"99,-145 0,-145 0,-130 99,-130 99,-145\"/>\n",
       "<text text-anchor=\"middle\" x=\"49.5\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sigma_alpha_post</text>\n",
       "</g>\n",
       "<!-- alpha -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>alpha</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"104.5\" cy=\"-57\" rx=\"29.795\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha</text>\n",
       "</g>\n",
       "<!-- sigma_alpha_post&#45;&gt;alpha -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>sigma_alpha_post&#45;&gt;alpha</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M54.7736,-129.7813C62.1692,-118.9569 75.9704,-98.757 87.1601,-82.3792\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"90.2993,-83.9889 93.0507,-73.7576 84.5195,-80.04 90.2993,-83.9889\"/>\n",
       "</g>\n",
       "<!-- mu_alpha_post -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>mu_alpha_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"201.5,-145 117.5,-145 117.5,-130 201.5,-130 201.5,-145\"/>\n",
       "<text text-anchor=\"middle\" x=\"159.5\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mu_alpha_post</text>\n",
       "</g>\n",
       "<!-- mu_alpha_post&#45;&gt;alpha -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>mu_alpha_post&#45;&gt;alpha</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M154.2264,-129.7813C146.8308,-118.9569 133.0296,-98.757 121.8399,-82.3792\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"124.4805,-80.04 115.9493,-73.7576 118.7007,-83.9889 124.4805,-80.04\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"227\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha ~ MultivariateNormal</text>\n",
       "<text text-anchor=\"start\" x=\"227\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mu_alpha_post : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"227\" y=\"-118.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sigma_alpha_post : PositiveDefinite()</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fdf5b273820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphical_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) Record example outputs of model and guide prior to training\n",
    "n_model_samples = 10\n",
    "n_guide_samples = 1000  \n",
    "\n",
    "predictive = pyro.infer.Predictive\n",
    "prior_predictive_pretrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_pretrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_predictive_pretrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Perform Inference\n",
    "\n",
    "We perform variational inference using `SVI` and `Trace_ELBO`.  \n",
    "Since we‚Äôre sampling many latent variables (2 per device), we increase:\n",
    "\n",
    "- `num_particles=20` ‚Üí reduces noise in the stochastic ELBO\n",
    "- `n_steps=1000` ‚Üí ensures convergence of many posterior parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 ; loss : 85108.50167531101\n",
      "epoch: 50 ; loss : 23693.886270862222\n",
      "epoch: 100 ; loss : 15907.15841288072\n",
      "epoch: 150 ; loss : 11680.203809162056\n",
      "epoch: 200 ; loss : 9550.65069953253\n",
      "epoch: 250 ; loss : 8026.226922328902\n",
      "epoch: 300 ; loss : 8055.070809938564\n",
      "epoch: 350 ; loss : 5762.561872763935\n",
      "epoch: 400 ; loss : 4529.060484628828\n",
      "epoch: 450 ; loss : 3564.994115249424\n",
      "epoch: 500 ; loss : 3392.7036936252207\n",
      "epoch: 550 ; loss : 3081.5770924279777\n",
      "epoch: 600 ; loss : 3221.23861320117\n",
      "epoch: 650 ; loss : 3074.4655887440586\n",
      "epoch: 700 ; loss : 3178.385993310437\n",
      "epoch: 750 ; loss : 2510.1895962380063\n",
      "epoch: 800 ; loss : 2489.295453861943\n",
      "epoch: 850 ; loss : 2403.3524367201553\n",
      "epoch: 900 ; loss : 2291.5813533590012\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    3. Perform inference\n",
    "\"\"\"\n",
    "\n",
    "# i) Set up inference\n",
    "adam = pyro.optim.Adam({\"lr\": 0.01})\n",
    "elbo = pyro.infer.Trace_ELBO(num_particles = 20)\n",
    "svi = pyro.infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "# ii) Perform svi\n",
    "data = (T_true, T_meas)\n",
    "loss_sequence = []\n",
    "for step in range(1000):\n",
    "    loss = svi.step(*data)\n",
    "    loss_sequence.append(loss)\n",
    "    if step %50 == 0:\n",
    "        print(f'epoch: {step} ; loss : {loss}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† ELBO in the Hierarchical Setting\n",
    "\n",
    "The Evidence Lower Bound (ELBO) now captures two terms:\n",
    "\n",
    "$$\n",
    "\\text{ELBO} = \\mathbb{E}_{q_\\phi(z)}[\\log p_\\theta(x, z)] - \\mathbb{E}_{q_\\phi(z)}[\\log q_\\phi(z)]\n",
    "$$\n",
    "\n",
    "Compared to earlier models:\n",
    "\n",
    "- The **likelihood term** requires marginalizing over per-device latent variables\n",
    "- The **KL divergence** penalizes deviation between the posterior guide $q_\\phi(z)$ and posterior $p(z|x)$\n",
    "\n",
    "Because $q$ and $p$ are multivariate, we model **correlations** between offset and scale per device.\n",
    "This allows more flexible and realistic posterior approximations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) Record example outputs of model and guide after training\n",
    "prior_predictive_posttrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_posttrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_predictive_posttrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n",
    "\n",
    "\n",
    "# iv) Additional investigations\n",
    "model_trace = pyro.poutine.trace(model).get_trace(T_true, observations = T_meas)\n",
    "guide_trace = pyro.poutine.trace(guide).get_trace(T_true, observations = T_meas)\n",
    "print('These are the shapes inside of the model : \\n {}'.format(model_trace.format_shapes()))\n",
    "print('These are the shapes inside of the guide : \\n {}'.format(guide_trace.format_shapes()))\n",
    "\n",
    "# The parameters of the posterior are again stored in pyro's param_store\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print('Param : {}; Value : {}'.format(name, value))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê Shape Overview\n",
    "\n",
    "| Object | Shape | Meaning |\n",
    "|--------|-------|---------|\n",
    "| `T_true` / `T_meas` | `[n_device, n_measure]` | True/measured temperatures |\n",
    "| `alpha` in model | `[n_device, 2]` | Latent offset and scale per device |\n",
    "| `mu_alpha_post` | `[n_device, 2]` | Posterior mean of $(\\alpha_0, \\alpha_1)$ |\n",
    "| `sigma_alpha_post` | `[n_device, 2, 2]` | Posterior covariance per device |\n",
    "| `posterior_samples` | `[n_samples, n_device, 2]` | Draws from the guide |\n",
    "| `posterior_predictive` | `[n_samples, n_device, n_measure]` | Simulated data conditioned on guide |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Interpretations and Illustrations\n",
    "\n",
    "### üîç Notes on Optimization\n",
    "\n",
    "- The ELBO loss is now **stochastic** and **noisy**\n",
    "- You should see **noisy downward trend** in the loss curve\n",
    "- Larger `num_particles` can help reduce variance, at the cost of runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    4. Interpretations and illustrations\n",
    "\"\"\"\n",
    "\n",
    "# i) Plot and print ELBO loss\n",
    "plt.figure(1, dpi = 300)\n",
    "plt.plot(loss_sequence)\n",
    "plt.yscale(\"log\")\n",
    "plt.title('ELBO loss during training (log scale)')\n",
    "plt.xlabel('Epoch nr')\n",
    "plt.ylabel('value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç A look at the results\n",
    "\n",
    "We now compare four things:\n",
    "\n",
    "1. üìä The real sensor measurements\n",
    "2. üé≤ Samples from the **prior predictive distribution**\n",
    "3. üéØ Samples from the **posterior predictive distribution**\n",
    "4. üîç Samples from the guide‚Äôs posterior $q(\\alpha)$ (before and after training)\n",
    "\n",
    "Each plot reveals how the model fits and adapts:\n",
    "\n",
    "- Posterior predictive samples after training should match real data better\n",
    "- The histogram of latent $(\\alpha_0, \\alpha_1)$ values shows how different sensors learn different calibrations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Compare model output and data\n",
    "\n",
    "# Create the figure and 2x5 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=False, sharey=False)\n",
    "# Global y-axis limits\n",
    "y_min = T_meas.min()\n",
    "y_max = T_meas.max()\n",
    "\n",
    "# FIRST ROW\n",
    "# First plot: measurement data\n",
    "for i in range(n_device):\n",
    "    axes[0,0].scatter(T_true[i,:], T_meas[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,0].set_title(\"Measurement data\")\n",
    "axes[0,0].set_xlabel(\"T_true\")\n",
    "axes[0,0].set_ylabel(\"T_meas\")\n",
    "axes[0,0].set_ylim(y_min, y_max)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Second plot: data produced by model pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,1].scatter(T_true[i,:], prior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,1].set_title(\"Data from model pre-training\")\n",
    "axes[0,1].set_xlabel(\"T_true\")\n",
    "axes[0,1].set_ylabel(\"T_meas\")\n",
    "axes[0,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,2].scatter(T_true[i,:], posterior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,2].set_title(\"Data from posterior predictive pre-training\")\n",
    "axes[0,2].set_xlabel(\"T_true\")\n",
    "axes[0,2].set_ylabel(\"T_meas\")\n",
    "axes[0,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide pre-training\n",
    "axes[0,3].hist2d(posterior_pretrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_pretrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[0,3].set_title(\"2D Histogram of parameters pre-train\")\n",
    "axes[0,3].set_xlabel(\"alpha_0\")\n",
    "axes[0,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "# SECOND ROW\n",
    "# Second plot: data produced by model post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,1].scatter(T_true[i,:], prior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,1].set_title(\"Data from model post-training\")\n",
    "axes[1,1].set_xlabel(\"T_true\")\n",
    "axes[1,1].set_ylabel(\"T_meas\")\n",
    "axes[1,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,2].scatter(T_true[i,:], posterior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,2].set_title(\"Data from posterior predictive post-training\")\n",
    "axes[1,2].set_xlabel(\"T_true\")\n",
    "axes[1,2].set_ylabel(\"T_meas\")\n",
    "axes[1,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide post-training\n",
    "axes[1,3].hist2d(posterior_posttrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_posttrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[1,3].set_title(\"2D Histogram of parameters post-train\")\n",
    "axes[1,3].set_xlabel(\"alpha_0\")\n",
    "axes[1,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Modeling Insight\n",
    "\n",
    "This model brings us closer to **real-world heterogeneity**:\n",
    "\n",
    "- Instead of assuming all sensors behave identically, we now let each sensor have its own calibration\n",
    "- Inference automatically **discovers sensor-specific behavior**\n",
    "- One sensor (visible in the 2D histogram) shows a strong **offset deviation**, suggesting a faulty sensor or outlier\n",
    "\n",
    "üìå This sets the stage for **outlier detection** and **robust modeling**, which will be introduced in the next notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßæ Summary\n",
    "\n",
    "### What Model 3 Adds\n",
    "\n",
    "| Feature | Description |\n",
    "|--------|-------------|\n",
    "| üß† Model | Latent offset & scale **per device**, sampled from a prior |\n",
    "| üß≠ Guide | One multivariate Gaussian per device for posterior over $(\\alpha_0, \\alpha_1)$ |\n",
    "| üìä Inference | SVI with `Trace_ELBO(num_particles=20)` |\n",
    "| üîé Posterior | 5 latent variables ‚Üí 5 guides ‚Üí each has mean + full covariance |\n",
    "| üìâ Loss behavior | ELBO gets noisier; convergence visible in posterior predictive |\n",
    "| üí° Takeaway | Model starts capturing **device-specific calibration** differences |\n",
    "\n",
    "\n",
    "### Central learning experience\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "‚úÖ Defined a linear model where **each device has its own latent parameters** (offset, scale)  \n",
    "‚úÖ Used **Multivariate Normal priors** and sampled per-device parameters in a plate  \n",
    "‚úÖ Built a guide using **per-device multivariate Normal distributions** (with full covariance)  \n",
    "‚úÖ Used **SVI** to infer the latent posterior for each device  \n",
    "‚úÖ Visualized **prior predictive**, **posterior predictive**, and **posterior samples**  \n",
    "‚úÖ Observed how **variational inference adapts to sensor heterogeneity**\n",
    "\n",
    "---\n",
    "\n",
    "üìå In the next notebook, we‚Äôll explore **discrete latent variables** to model **outlier sensors**  \n",
    "(e.g., a switch that turns a faulty sensor on/off) using categorical or Bernoulli latent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
