{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro Crash Course – Notebook 6: Detecting Faulty Devices with Discrete Latents\n",
    "\n",
    "This notebook introduces **discrete latent variables** into our probabilistic model. We classify each sensor as either **faulty or functioning** based on its measurements, extending the hierarchical Bayesian model from Notebook 5.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔙 Quick Recap\n",
    "\n",
    "*Notebook 5* gave each sensor its own offset and scale, inferred from data using variational inference. Now we assume **most sensors are good**, but a few might be **faulty** — with abnormally uncertain parameters.\n",
    "\n",
    "We model this using a **latent binary switch `is_faulty` per device**, controlling which prior distribution the sensor’s parameters (`alpha`) are drawn from.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Learning Goals\n",
    "\n",
    "- Introduce **discrete latent variables** and their use for classification\n",
    "- Learn to model **qualitatively different behaviors** (e.g., normal vs. faulty sensors)\n",
    "- Use **enumeration-based inference** (`TraceEnum_ELBO`) to marginalize over discrete variables\n",
    "- Diagnose outliers by inspecting **posterior probabilities** over `is_faulty`\n",
    "- Maintain structured calibration with **per-device posterior distributions**\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Modeling Idea\n",
    "\n",
    "Each device is assumed to be either:\n",
    "- ✅ **Functioning normally**: parameters drawn from `N([0, 1], Σ)`\n",
    "- ❌ **Faulty**: parameters drawn from `N([0, 1], Σ_faulty)` with much larger variance\n",
    "\n",
    "The binary latent `is_faulty ∈ {0, 1}` is drawn from a Bernoulli prior with small `p_faulty`.   It controls whether the model treats the device as uncertain or reliable.\n",
    "\n",
    "This latent variable is inferred jointly with the usual offset and scale, and allows posterior classification after training.\n",
    "\n",
    "\n",
    "📌 In the next notebook, we’ll go one step further:  \n",
    "Use **neural networks inside the probabilistic model**, letting us build **deep calibration models** or **learn nonlinear effects**.\n",
    "\n",
    "\n",
    "For this, do the following:\n",
    "   1. Imports and definitions  \n",
    "   2. Build model and guide  \n",
    "   3. Perform inference  \n",
    "   4. Interpretations and illustrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/atlasoptimization/tutorials/blob/master/pyro/pyro_crash_course/pyro_cc_6_model_4.ipynb\n",
    ")\n",
    "\n",
    "> ⚠️ You do *not* need to sign in with GitHub to run this notebook.  \n",
    "> Just click the Colab badge and start executing.\n",
    "\n",
    "📎 Input: `sensor_measurements.csv` (generated in `pyro_cc_1_hello_dataset`) \n",
    "\n",
    "📌 Next: `pyro_cc_7_model_5` introduces some nonlinear trend on top of what we build here. That one will be the last tutorial of this crash course!\n",
    "\n",
    "\n",
    "This notebook and series are created for educational purposes by  [Dr. Jemil Avers Butt, Atlas Optimization GmbH](https://www.atlasoptimization.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start of Notebook 6\n",
    "\n",
    "## 1. Imports and definitions\n",
    "\n",
    "We now need to import the decorator @config_enumerate from pyro.infer. That decorator marks all discrete latents in model and guide for complete enumeration. This is necessary for inference with TraceEnum_ELBO, the corresponding ELBO loss function in the presence of discrete latent variables.\n",
    "\n",
    "\n",
    "📌 This is the first notebook that **introduces per-device latent variables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. Imports and definitions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# i) Imports\n",
    "# If you run this in Colab, you might get an import error as pyro is not\n",
    "# installed by default. In that case, uncomment the following command.\n",
    "# !pip install pyro-ppl\n",
    "\n",
    "# We need an additional decoration config_enumerate to declare inference over\n",
    "# a discrete latent variable - then the appropriate sample sites are automatically\n",
    "# exxhaustively enumerated.\n",
    "import pyro\n",
    "import torch\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pyro.infer import config_enumerate\n",
    "\n",
    "\n",
    "# ii) Definitions\n",
    "\n",
    "# Read csv, infer dimensions\n",
    "# !wget https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
    "measurement_df = pandas.read_csv(\"sensor_measurements.csv\")\n",
    "n_device = measurement_df[\"sensor_id\"].nunique()\n",
    "n_measure = measurement_df[\"time_step\"].nunique()\n",
    "\n",
    "# Read out T_true and T_meas: the true temperature and the measured temperature\n",
    "T_true = torch.tensor(measurement_df[\"T_true\"].values).reshape(n_device, n_measure)\n",
    "T_meas = torch.tensor(measurement_df[\"T_measured\"].values).reshape(n_device, n_measure)\n",
    "\n",
    "# Assume standard deviation\n",
    "sigma_T_meas = 0.3\n",
    "\n",
    "# Fix random seed\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Build model and guide\n",
    "\n",
    "## 🔧 Model Specification\n",
    "\n",
    "We assume each sensor can be either:\n",
    "\n",
    "- **Functioning normally**: Parameters drawn from `N([0, 1], Σ)`\n",
    "- **Faulty**: Parameters drawn from `N([0, 1], Σ_faulty)` with much larger variance\n",
    "\n",
    "Each device gets its own binary latent variable `is_faulty ~ Bernoulli(p_faulty)`.  \n",
    "The device’s calibration parameters `alpha = (offset, scale)` are drawn from a prior  \n",
    "that depends on whether it is faulty:\n",
    "\n",
    "\\[\n",
    "\\Sigma_{\\text{device}} = \\Sigma_{\\text{normal}} + \\texttt{is\\_faulty} \\cdot (\\Sigma_{\\text{faulty}} - \\Sigma_{\\text{normal}})\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\alpha_i \\sim \\mathcal{N}(\\mu_\\alpha, \\Sigma_{\\text{device},i})\n",
    "\\quad\\text{with}\\quad\n",
    "\\mu_\\alpha = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "The measurement model is unchanged:\n",
    "\n",
    "\\[\n",
    "T_{\\text{meas}}[i, j] \\sim \\mathcal{N}(\\alpha_{i,0} + \\alpha_{i,1} \\cdot T_{\\text{true}}[i, j], \\sigma_{\\text{Tmeas}})\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2. Build model and guide\n",
    "\"\"\"\n",
    "\n",
    "# i) Define the model\n",
    "# We implement the same model as in the previous notebook but add an additional\n",
    "# generative step in the beginning, where we sample from the device production\n",
    "# distribution to determine if the device is being produced faulty.\n",
    "\n",
    "# Define priors and fixed params\n",
    "mu_alpha = torch.tensor([0.0, 1.0]).expand(n_device, -1)\n",
    "Sigma_alpha = torch.tensor([[0.1**2,0], [0, 0.1**2]])\n",
    "p_faulty = 0.05\n",
    "Sigma_faulty = 100*Sigma_alpha\n",
    "\n",
    "# Build the model in pyro; use decorator to mark discrete variables for enumeration.\n",
    "@config_enumerate\n",
    "def model(input_vars = T_true, observations = None):\n",
    "    # Build reusable independence context device_plate.\n",
    "    device_plate = pyro.plate('device_plate', size=n_device, dim=-1)\n",
    "\n",
    "    with device_plate:\n",
    "        is_faulty = pyro.sample('is_faulty', pyro.distributions.Bernoulli(p_faulty))\n",
    "\n",
    "    # Convert boolean indicator to tensor of shape [n_device, 1, 1] for broadcasting\n",
    "    is_faulty_tensor = is_faulty.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    # Build Sigma_device via interpolation\n",
    "    Sigma_device = Sigma_alpha + is_faulty_tensor * (Sigma_faulty - Sigma_alpha)  # shape: [n_device, 2, 2]\n",
    "\n",
    "    # Make alpha_dist have a sigma dependent on  device and is_faulty\n",
    "    alpha_dist = pyro.distributions.MultivariateNormal(\n",
    "        loc=mu_alpha,                   # shape: [2, n_device, 2]\n",
    "        covariance_matrix=Sigma_device)     # shape: [2, n_device, 2, 2]\n",
    "            \n",
    "    # Independence in first dim of alpha_dist, sample n_device times\n",
    "    with device_plate:\n",
    "        alpha = pyro.sample(\"alpha\", alpha_dist)\n",
    "    mean_obs = alpha[:, 0].unsqueeze(1) + alpha[:, 1].unsqueeze(1) * T_true\n",
    "    obs_dist = pyro.distributions.Normal(loc=mean_obs, scale=sigma_T_meas)\n",
    "\n",
    "    # Sample from this distribution and declare the samples independent in the\n",
    "    with pyro.plate('device_plate', dim=-2):\n",
    "        with pyro.plate('measure_plate', dim=-1):\n",
    "            pyro.sample(\"observations\", obs_dist, obs=observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 Guide Construction\n",
    "\n",
    "We use a **mean-field variational approximation** over the latent variables:\n",
    "\n",
    "- A learnable Bernoulli posterior for `is_faulty`, with device-specific probabilities\n",
    "- A multivariate Gaussian posterior for each device’s calibration parameters\n",
    "\n",
    "\\[\n",
    "q_\\phi(\\alpha_i, \\texttt{is\\_faulty}_i) = \n",
    "\\text{Bernoulli}(\\pi_i) \\cdot \n",
    "\\mathcal{N}(\\mu_{\\alpha,i}, \\Sigma_{\\alpha,i})\n",
    "\\]\n",
    "\n",
    "Both \\(\\pi_i\\) and the parameters of the Gaussian are trained during inference.\n",
    "\n",
    "Enumerating `is_faulty` is handled automatically using `@config_enumerate`  \n",
    "and `TraceEnum_ELBO`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Build the guide\n",
    "# Since the guide needs to encode the posterior distribution of the is_faulty\n",
    "# variable now as well, we build a distribution with trainable parameters. \n",
    "\n",
    "# Build the guide\n",
    "@config_enumerate\n",
    "def guide(input_vars = T_true, observations = None):\n",
    "    # Build reusable independence context device_plate.\n",
    "    device_plate = pyro.plate('device_plate', size=n_device, dim=-1)\n",
    "    \n",
    "    # Learn a per-device probability of being faulty\n",
    "    probs_faulty = pyro.param(\"probs_faulty\", 0.05 * torch.ones(n_device),\n",
    "                              constraint=pyro.distributions.constraints.unit_interval)\n",
    "    \n",
    "    # Sample discrete variable from learned probabilities\n",
    "    with device_plate:\n",
    "        is_faulty = pyro.sample(\"is_faulty\", pyro.distributions.Bernoulli(probs_faulty))\n",
    "    \n",
    "    # Per-device means and scales (shape [n_device, 2])\n",
    "    mu_alpha_post = pyro.param('mu_alpha_post', torch.tensor([0,1.0]).unsqueeze(0).expand([n_device,2]))\n",
    "    sigma_alpha_post = (pyro.param('sigma_alpha_post', 0.1 * (torch.eye(2).unsqueeze(0)).expand([n_device,2,2]),\n",
    "                             constraint=pyro.distributions.constraints.positive_definite) \n",
    "                        + 0.001 * torch.eye(2))\n",
    "    # We add 1e-3 on the diagonal of the covariance matrix to avoid numerical issues\n",
    "    # related to positive definiteness tests.\n",
    "\n",
    "    with device_plate:\n",
    "        # Multivariate Gaussian for each device (allow for correlations)\n",
    "        alpha = pyro.sample('alpha', pyro.distributions.MultivariateNormal(loc = mu_alpha_post,\n",
    "                                        covariance_matrix = sigma_alpha_post))\n",
    "\n",
    "    return alpha, is_faulty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📐 Tensor Shapes Table\n",
    "\n",
    "| Variable              | Shape                 | Description |\n",
    "|-----------------------|----------------------|-------------|\n",
    "| `T_true`              | `[n_device, n_measure]` | True temperature values (input) |\n",
    "| `T_meas`              | `[n_device, n_measure]` | Observed noisy measurements |\n",
    "| `alpha`               | `[n_device, 2]`         | Offset and scale per device |\n",
    "| `mu_alpha_post`       | `[n_device, 2]`         | Posterior means of `alpha` |\n",
    "| `sigma_alpha_post`    | `[n_device, 2, 2]`       | Posterior covariances of `alpha` |\n",
    "| `is_faulty`           | `[n_device]`            | Binary latent variable for each device |\n",
    "| `Sigma_device`        | `[n_device, 2, 2]`       | Covariance matrix per device depending on `is_faulty` |\n",
    "| `posterior_faulty`    | `[n_samples, n_device]`  | Posterior samples of `is_faulty` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) Illustrate model and guide\n",
    "# Now the illustration of the model shows alpha inside of the device plate as\n",
    "# a multivariate normal random variable. In the guide we now have also alpha\n",
    "# as a member of the device plate and affected by the posterior mean and covariance.\n",
    "graphical_model = pyro.render_model(model = model, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "graphical_guide = pyro.render_model(model = guide, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphical_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) Record example outputs of model and guide prior to training\n",
    "n_model_samples = 10\n",
    "n_guide_samples = 1000  \n",
    "\n",
    "predictive = pyro.infer.Predictive\n",
    "prior_predictive_pretrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_pretrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_pretrain_faulty = predictive(guide, num_samples = n_guide_samples)()['is_faulty']\n",
    "posterior_predictive_pretrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Perform inference\n",
    "\n",
    "## 🔁 Inference with Discrete Latents\n",
    "\n",
    "To train the model, we minimize the **Evidence Lower Bound (ELBO)** using stochastic variational inference:\n",
    "\n",
    "\\[\n",
    "\\text{ELBO} = \\mathbb{E}_{q_\\phi(z)}[\\log p(x, z) - \\log q_\\phi(z)]\n",
    "\\]\n",
    "\n",
    "Key points:\n",
    "\n",
    "- Discrete variables are enumerated (i.e., marginalized exactly)\n",
    "- Continuous variables (like `alpha`) are sampled using Monte Carlo\n",
    "- We use `TraceEnum_ELBO` with `num_particles > 1` to reduce noise\n",
    "\n",
    "This allows us to **learn both**:\n",
    "- The calibration parameters per device\n",
    "- The likelihood of each device being faulty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    3. Perform inference\n",
    "\"\"\"\n",
    "\n",
    "# i) Set up inference\n",
    "# In terms of loss function, we need now TraceEnum_ELBO instead of Trace_ELBO,\n",
    "# as the latter one is not designed to handle enumeration.\n",
    "adam = pyro.optim.Adam({\"lr\": 0.01})\n",
    "elbo = pyro.infer.TraceEnum_ELBO(num_particles = 20,\n",
    "                                 max_plate_nesting = 2)\n",
    "svi = pyro.infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "# ii) Perform svi\n",
    "data = (T_true, T_meas)\n",
    "loss_sequence = []\n",
    "for step in range(1000):\n",
    "    loss = svi.step(*data)\n",
    "    loss_sequence.append(loss)\n",
    "    if step %50 == 0:\n",
    "        print(f'epoch: {step} ; loss : {loss}')\n",
    "                  \n",
    "# iii) Record example outputs of model and guide after training\n",
    "prior_predictive_posttrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_posttrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_posttrain_faulty = predictive(guide, num_samples = n_guide_samples)()['is_faulty']\n",
    "posterior_predictive_posttrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Posterior Interpretation\n",
    "\n",
    "Once training is complete:\n",
    "\n",
    "- We extract samples of `alpha` from the guide to inspect per-device estimates\n",
    "- We estimate **posterior probabilities** for `is_faulty` by sampling it repeatedly\n",
    "- A **higher posterior probability** means a sensor is more likely to be faulty\n",
    "\n",
    "This can be used to **flag outliers or malfunctioning devices**  \n",
    "based on learned probabilistic evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) Additional investigations\n",
    "model_trace = pyro.poutine.trace(model).get_trace(T_true, observations = T_meas)\n",
    "guide_trace = pyro.poutine.trace(guide).get_trace(T_true, observations = T_meas)\n",
    "print('These are the shapes inside of the model : \\n {}'.format(model_trace.format_shapes()))\n",
    "print('These are the shapes inside of the guide : \\n {}'.format(guide_trace.format_shapes()))\n",
    "\n",
    "# The parameters of the posterior are again stored in pyro's param_store\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print('Param : {}; Value : {}'.format(name, value))\n",
    "\n",
    "# Estimate the posterior probabilities of each device being faulty. \n",
    "faulty_pretrain = torch.mean(posterior_pretrain_faulty,dim=0)\n",
    "faulty_posttrain = torch.mean(posterior_posttrain_faulty,dim=0)\n",
    "print('is_faulty ground truth : [1, 0, 0, 0, 0]')\n",
    "print('Probabilities of being faulty pre-training : {} \\n'\n",
    "      'Probabilities of being faulty post-training : {} '\n",
    "      .format(faulty_pretrain, faulty_posttrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Illustrations and interpretations\n",
    "\n",
    "We visualize:\n",
    "\n",
    "- Raw data alongside generated model outputs\n",
    "- Posterior samples before and after training\n",
    "- Estimated latent values (fault probabilities) per device\n",
    "\n",
    "This provides immediate insight into how well the model fits each sensor and which sensors deviate significantly from expected behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    4. Interpretations and illustrations\n",
    "\"\"\"\n",
    "\n",
    "# i) Plot and print ELBO loss\n",
    "plt.figure(1, dpi = 300)\n",
    "plt.plot(loss_sequence)\n",
    "plt.yscale(\"log\")\n",
    "plt.title('ELBO loss during training (log scale)')\n",
    "plt.xlabel('Epoch nr')\n",
    "plt.ylabel('value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ii) Compare model output and data\n",
    "\n",
    "# Create the figure and 2x5 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=False, sharey=False)\n",
    "# Global y-axis limits\n",
    "y_min = T_meas.min()\n",
    "y_max = T_meas.max()\n",
    "\n",
    "# FIRST ROW\n",
    "# First plot: measurement data\n",
    "for i in range(n_device):\n",
    "    axes[0,0].scatter(T_true[i,:], T_meas[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,0].set_title(\"Measurement data\")\n",
    "axes[0,0].set_xlabel(\"T_true\")\n",
    "axes[0,0].set_ylabel(\"T_meas\")\n",
    "axes[0,0].set_ylim(y_min, y_max)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Second plot: data produced by model pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,1].scatter(T_true[i,:], prior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,1].set_title(\"Data from model pre-training\")\n",
    "axes[0,1].set_xlabel(\"T_true\")\n",
    "axes[0,1].set_ylabel(\"T_meas\")\n",
    "axes[0,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,2].scatter(T_true[i,:], posterior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,2].set_title(\"Data from posterior predictive pre-training\")\n",
    "axes[0,2].set_xlabel(\"T_true\")\n",
    "axes[0,2].set_ylabel(\"T_meas\")\n",
    "axes[0,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide pre-training\n",
    "axes[0,3].hist2d(posterior_pretrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_pretrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[0,3].set_title(\"2D Histogram of parameters pre-train\")\n",
    "axes[0,3].set_xlabel(\"alpha_0\")\n",
    "axes[0,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "# SECOND ROW\n",
    "# Second plot: data produced by model post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,1].scatter(T_true[i,:], prior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,1].set_title(\"Data from model post-training\")\n",
    "axes[1,1].set_xlabel(\"T_true\")\n",
    "axes[1,1].set_ylabel(\"T_meas\")\n",
    "axes[1,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,2].scatter(T_true[i,:], posterior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,2].set_title(\"Data from posterior predictive post-training\")\n",
    "axes[1,2].set_xlabel(\"T_true\")\n",
    "axes[1,2].set_ylabel(\"T_meas\")\n",
    "axes[1,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide post-training\n",
    "axes[1,3].hist2d(posterior_posttrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_posttrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[1,3].set_title(\"2D Histogram of parameters post-train\")\n",
    "axes[1,3].set_xlabel(\"alpha_0\")\n",
    "axes[1,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧾 Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "✅ Built a hierarchical model with **per-device calibration parameters**  \n",
    "✅ Added a **discrete latent variable** to detect faulty sensors  \n",
    "✅ Used **enumeration-based inference** to marginalize out `is_faulty`  \n",
    "✅ Visualized prior/posterior predictive samples and fault probabilities  \n",
    "✅ Showed that **faulty devices can be identified** automatically via inference\n",
    "\n",
    "\n",
    "📌 In the next notebook, we’ll go one step further:  \n",
    "Use **neural networks inside the probabilistic model**, letting us build **deep calibration models** or **learn nonlinear effects**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
