{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro Crash Course ‚Äì Notebook 3: Linear Calibration with Trainable Parameters\n",
    "\n",
    "This notebook introduces first trainable parameters to the probabilistic model for sensor data ‚Äî thereby showcasing meaningful inference with svi.\n",
    "\n",
    "### üîô Quick Recap\n",
    "\n",
    "*Notebook 2* treated $T_{\\text{meas}}$ as $T_{\\text{true}}$ plus fixed noise.  \n",
    "No parameters ‚áí no learning.  \n",
    "Today we let the model **adjust a global offset and scale** to better fit the data.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Learning Goals\n",
    "\n",
    "- Introduce **deterministic trainable parameters** with `pyro.param`\n",
    "- Use SVI to perform **Maximum Likelihood Estimation** (equivalent to least-squares)\n",
    "- Compare Pyro‚Äôs learned parameters with a classic least-squares solution\n",
    "- Diagnose model fit via ELBO curve and residual plots\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "For this, do the following:\n",
    "   1. Imports and definitions\n",
    "   2. Build model and guide\n",
    "   3. Perform inference\n",
    "   4. Interpretations and illustrations\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/atlasoptimization/tutorials/blob/master/pyro/pyro_crash_course/pyro_cc_3_model_1.ipynb\n",
    ")\n",
    "\n",
    "> ‚ö†Ô∏è You do *not* need to sign in with GitHub to run this notebook.  \n",
    "> Just click the Colab badge and start executing.\n",
    "\n",
    "üìé Input: `sensor_measurements.csv` (generated in `pyro_cc_1_hello_dataset`) \n",
    "\n",
    "üìå Next: `pyro_cc_4_model_2` swaps trainable parameters for unobserved latent variable to build something actually bayesian\n",
    "\n",
    "\n",
    "This notebook and series are created for educational purposes by  [Dr. Jemil Avers Butt, Atlas Optimization GmbH](https://www.atlasoptimization.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Notebook 3\n",
    "\n",
    "## 1. Imports and definitions\n",
    "\n",
    "We import:\n",
    "\n",
    "- `pandas` to read the dataset generated in the previous notebook\n",
    "- `pyro` and `torch` to build the model and run inference\n",
    "- `matplotlib` for plotting\n",
    "\n",
    "We also **infer the shape** of the dataset (`n_device`, `n_measure`) and load the tensors for true and measured temperatures.\n",
    "\n",
    "üìå This is the first notebook that **reuses output from earlier in the course**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. Imports and definitions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# i) Imports\n",
    "# If you run this in Colab, you might get an import error as pyro is not\n",
    "# installed by default. In that case, uncomment the following command.\n",
    "# !pip install pyro-ppl\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "import pandas\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ii) Definitions\n",
    "\n",
    "# Read csv, infer dimensions\n",
    "!wget https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
    "measurement_df = pandas.read_csv(\"sensor_measurements.csv\")\n",
    "n_device = measurement_df[\"sensor_id\"].nunique()\n",
    "n_measure = measurement_df[\"time_step\"].nunique()\n",
    "\n",
    "# Read out T_true and T_meas: the true temperature and the measured temperature\n",
    "T_true = torch.tensor(measurement_df[\"T_true\"].values).reshape(n_device, n_measure)\n",
    "T_meas = torch.tensor(measurement_df[\"T_measured\"].values).reshape(n_device, n_measure)\n",
    "\n",
    "# Assume standard deviation\n",
    "sigma_T_meas = 0.3\n",
    "\n",
    "# Fix random seed\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Build Model and Guide\n",
    "\n",
    "We now define a **simple linear model** where all sensors share the same **global calibration parameters**: a bias $\\alpha_0$ and a scale factor $\\alpha_1$.\n",
    "\n",
    "We assume that measured temperature is a noisy linear transformation of the true temperature:\n",
    "\n",
    "$$\n",
    "T_{\\text{measured}} = \\alpha_0 + \\alpha_1 \\cdot T_{\\text{true}} + \\text{Noise}\n",
    "$$\n",
    "\n",
    "which translates into a probabilistic model:\n",
    "\n",
    "$$\n",
    "T_{\\text{measured}} \\sim \\mathcal{N}(\\alpha_0 + \\alpha_1 \\cdot T_{\\text{true}}, \\; \\sigma)\n",
    "$$\n",
    "\n",
    "* **Trainable parameters:** $\\alpha_0$, $\\alpha_1$ are declared using `pyro.param`, which marks them for optimisation during inference.  \n",
    "* Still **no latent variables**, so the guide remains empty.  \n",
    "* We treat all observations as independent over two dimensions: **device** and **measurement index**.  \n",
    "* This is the first model that performs **actual learning** using gradients of the ELBO.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Modeling Insight\n",
    "\n",
    "This model introduces **deterministic parameters** into the probabilistic framework. By placing $\\alpha_0$ and $\\alpha_1$ directly in the mean of the likelihood, we enable Pyro to **learn a best-fit linear mapping** from $T_{\\text{true}}$ to $T_{\\text{measured}}$.\n",
    "\n",
    "Although this still assumes all sensors behave identically, it improves upon the previous model and lays the groundwork for more complex, sensor-specific models.\n",
    "\n",
    "The **guide remains empty** since all uncertainty in the model is captured by the known noise distribution ‚Äî there are no latent random variables to infer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2. Build model and guide\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# i) Define the model\n",
    "# We recreate the simple probabilistic model  T_meas ~ N(mu_T_meas, sigma_T_meas)\n",
    "# with mu_T_meas = offset + scale * T_true.\n",
    "\n",
    "\n",
    "# Build the model in pyro\n",
    "def model(input_vars = T_true, observations = None):\n",
    "    # Call pyro.param to declare parameters alpha_0 and alpha_1, which are to\n",
    "    # pyro deterministic unknown tensors marked by pyro for optimization during\n",
    "    # later inference. Inputs to pyro.param are a unique name and a init value.\n",
    "    alpha_0 = pyro.param('alpha_0', torch.zeros([]))\n",
    "    alpha_1 = pyro.param('alpha_1', torch.ones([]))\n",
    "    \n",
    "    # Build the observation distribution using the parameters. Note that the \n",
    "    # alpha's are torch.tensors and broadcast over T_true. The distribution\n",
    "    # obs_dist produces therefore values of shape [n_device, n_measure].\n",
    "    obs_dist = pyro.distributions.Normal(loc = alpha_0 + alpha_1 * T_true,\n",
    "                                         scale = sigma_T_meas)\n",
    "    \n",
    "    # Sample from this distribution and declare the samples independent in the\n",
    "    # first two dims. \n",
    "    with pyro.plate('device_plate', dim = -2):\n",
    "        with pyro.plate('measure_plate', dim = -1):\n",
    "            obs = pyro.sample('observations', obs_dist, obs = observations)\n",
    "    \n",
    "    return obs\n",
    "\n",
    "# ii) Build the guide\n",
    "\n",
    "def guide(input_vars = T_true, observations = None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Pyro Mechanics\n",
    "\n",
    "| Primitive         | Role in this notebook | New vs. previous model |\n",
    "|-------------------|-----------------------|------------------------|\n",
    "| `pyro.param`      | Registers $\\alpha_0,\\;\\alpha_1$ for gradient-based optimisation | **New** |\n",
    "| `pyro.sample`     | Samples / scores observations under $\\mathcal N(\\alpha_0+\\alpha_1T,\\;\\sigma)$ | Same dual use |\n",
    "| `pyro.plate`      | Marks **device** and **measurement** dims as i.i.d. | Same |\n",
    "\n",
    "Because parameters appear **inside** the mean, gradients of the ELBO now flow to them, enabling learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîç Investigating the Model\n",
    "\n",
    "We again use `pyro.render_model()` to generate a **plate diagram** of the model. This provides a quick sanity check of the model‚Äôs structure:\n",
    "\n",
    "- Random variables are shown as ellipses (here: observations)  \n",
    "- Plates indicate **independence contexts** (over devices and measurements)  \n",
    "- Parameters (here: $\\alpha_0$, $\\alpha_1$) are shown together with where they impact the model \n",
    "\n",
    "### üßµ Execution Trace and Log-Probabilities\n",
    "\n",
    "We then run the model under `pyro.poutine.trace` to capture the **execution trace**, which is a summary dictionary that contains everything important and represents the model state internally. In there you find entries for:\n",
    "\n",
    "- Sampled values and their shapes  \n",
    "- Parameters used  \n",
    "- Log-probabilities (if computed)\n",
    "\n",
    "This trace allows us to:\n",
    "\n",
    "- Inspect the tensor shapes of all involved components (`format_shapes`)  \n",
    "- Access all stochastic nodes, deterministic computations, and parameters  \n",
    "- Manually retrieve the **log-probability** of the observed data, which should match the ELBO loss (since the guide is empty)\n",
    "\n",
    "You can compute the total log-likelihood of the data in two ways:\n",
    "\n",
    "1. **By hand**, using the known `Normal` distribution and learned parameters  \n",
    "2. **From the trace**, after calling `trace.compute_log_prob()`\n",
    "\n",
    "These two values should match exactly ‚Äî this serves as a good internal consistency check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Additional investigations\n",
    "\n",
    "graphical_model = pyro.render_model(model = model, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "graphical_model\n",
    "\n",
    "# We can inspect the model using pyro functionality. For this, we let the model\n",
    "# run once and record the trace, a representation of everything important.\n",
    "model_trace = pyro.poutine.trace(model).get_trace(T_true, observations = T_meas)\n",
    "print('These are the shapes of the involved objects : \\n{} \\nFormat: batch_shape,'\\\n",
    "      ' event_shape'.format(model_trace.format_shapes()))\n",
    "# All info is contained in the model nodes:\n",
    "print(model_trace.nodes)\n",
    "\n",
    "# The parameters are stored in pyro's param_store and can be accessed by name\n",
    "alpha_0 = pyro.get_param_store()['alpha_0']\n",
    "alpha_1 = pyro.get_param_store()['alpha_1']\n",
    "   \n",
    "# We can again compute the log probs by hand to evaluate the fit\n",
    "obs_dist = pyro.distributions.Normal(loc = alpha_0 + alpha_1* T_true,\n",
    "                                     scale = sigma_T_meas)\n",
    "log_prob = torch.sum(obs_dist.log_prob(T_meas))\n",
    "\n",
    "# Note: the log_prob values are also accessible by inspection of the model trace.\n",
    "model_trace.compute_log_prob()\n",
    "log_prob_from_trace = torch.sum(model_trace.nodes[\"observations\"][\"log_prob\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Perform Inference\n",
    "\n",
    "We use `pyro.infer.SVI` with the **standard ELBO loss** and an **empty guide**. Because the model includes two trainable parameters ‚Äî $\\alpha_0$ and $\\alpha_1$ ‚Äî the ELBO is now **optimised with respect to these parameters** using gradient descent.\n",
    "\n",
    "We run a standard SVI training loop, tracking the loss at each step. This loss now reflects the current model fit.\n",
    "\n",
    "### üî¨ ELBO in This Case\n",
    "\n",
    "Because there are **no latent variables**, the ELBO reduces to the **negative log-likelihood** of the observed data under the current parameter values:\n",
    "\n",
    "$$\n",
    "\\text{ELBO}(\\alpha_0, \\alpha_1) = -\\log p_\\theta(x)\n",
    "= -\\sum_i \\log \\mathcal{N}(x_i \\mid \\alpha_0 + \\alpha_1 \\cdot T_{\\text{true},i}, \\sigma)\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "- The ELBO is now **a function of** $(\\alpha_0, \\alpha_1)$  \n",
    "- Minimising the ELBO corresponds exactly to performing **maximum likelihood estimation**  \n",
    "- For this simple Gaussian model, this is equivalent to solving a **least-squares regression**\n",
    "\n",
    "In the next notebook, where latent variables are introduced, the ELBO will also include the Kullback Leibler divergence terms measuring posterior approximation quality. For the current model it still remains a pure likelihood objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    3. Perform inference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# i) Set up inference\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.1})\n",
    "elbo = pyro.infer.Trace_ELBO()\n",
    "svi = pyro.infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "# Record example output of the model prior to training\n",
    "model_data_pretraining = copy.copy(model(T_true)).detach().numpy()\n",
    "\n",
    "\n",
    "# ii) Perform svi\n",
    "# Now some meaningful optimization happens. The gradients of the ELBO w.r.t. the\n",
    "# parameters are computed and the params are adjusted to decrease the ELBO loss.\n",
    "\n",
    "data = (T_true, T_meas)\n",
    "loss_sequence = []\n",
    "for step in range(300):\n",
    "    loss = svi.step(*data)\n",
    "    loss_sequence.append(loss)\n",
    "    if step %10 == 0:\n",
    "        print(f'epoch: {step} ; loss : {loss}')\n",
    "        \n",
    "# Record example output of the model after training\n",
    "model_data_posttraining = copy.copy(model(T_true)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê Sanity Check: Least-Squares vs. Pyro\n",
    "\n",
    "For linear models with Gaussian noise, the MLE equals the closed-form least-squares estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = (A^\\top A)^{-1}A^\\top y,\n",
    "$$\n",
    "\n",
    "where $A = \\begin{bmatrix}1 & T_{\\text{true}}\\end{bmatrix}$ stacked over all observations. We compute both solutions to verify they match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we perform least squares to estimate the coefficients alpha_0, alpha_1,\n",
    "# the result will coincide with what pyro produces.\n",
    "A = torch.vstack((torch.ones([n_device*n_measure]), T_true.flatten())).T\n",
    "alpha_ls = torch.linalg.pinv(A.T@A)@A.T@T_meas.flatten()\n",
    "print('The least squares solution is alpha_0 = {}, alpha_1 = {};\\n'\n",
    "      'The pyro solution is alpha_0 = {}, alpha_1 = {}'.format(alpha_ls[0], alpha_ls[1], alpha_0, alpha_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Interpretations and Illustrations\n",
    "\n",
    "We visually and quantitatively assess the model by comparing:\n",
    "\n",
    "- The **measured data** from the dataset  \n",
    "- The **samples generated by the model**  \n",
    "  ‚Äì both **before** and **after** training  \n",
    "\n",
    "This comparison helps us understand how the model evolves through inference.\n",
    "\n",
    "### üìà ELBO Curve\n",
    "\n",
    "We track the ELBO loss over training epochs (on a log scale). Since there are no latent variables, the ELBO corresponds to the **negative log-likelihood** of the data under the current parameter values. As training progresses, we expect the loss to **decrease steadily** and eventually **plateau** once optimal parameters are reached.\n",
    "\n",
    "Note that the ELBO is evaluated first for the model with the initial values for $\\alpha_0, \\alpha_1$ which are 0,1 respectively. Thereby the initial model corresponds to $\\T_{meas} \\sim \\mathcal{N}(T_{true}, \\sigma)$ - which is exactly our model_0. As a consequence, the first value of the ELBo should be exactly the same as for model_0 and then subsequent parameter adjustments decrease it. But not by much - the model is still far from sufficient to capture our datas complexities.\n",
    "\n",
    "### üéØ Visual Fit\n",
    "\n",
    "The scatter plots let us visually compare:\n",
    "\n",
    "1. The **true measurements**\n",
    "2. The model‚Äôs predictions **before training** (i.e. at default parameters)\n",
    "3. The model‚Äôs predictions **after training** (i.e. after optimisation)\n",
    "\n",
    "While this model is still limited ‚Äî it applies the same $\\alpha_0, \\alpha_1$ to all sensors ‚Äî we see a **slight improvement** in the alignment of predicted and observed measurements after training. This is also visible in the ELBO.\n",
    "\n",
    "### üßÆ Residual Analysis\n",
    "\n",
    "We further quantify the improvement by plotting histograms of the **residuals**:\n",
    "\n",
    "- **Pre-training residuals:** how far off the untrained model is  \n",
    "- **Post-training residuals:** should be narrower and more centered\n",
    "\n",
    "This gives a concrete sense of how the model's output distribution aligns with observed data. We see here that this simple model had only marginal success in explaining our data. Now, on average our residuals are 0 but still the multimodal nature and widespread distribution of the residuals is not satisfactory.\n",
    "\n",
    "> ‚ö†Ô∏è The model still **cannot** capture device-specific biases or nonlinear trends. Later models will address these limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    4. Interpretations and illustrations\n",
    "\"\"\"\n",
    "\n",
    "# i) Plot and print ELBO loss\n",
    "\n",
    "# The ELBO is simply the - log evidence = - log probability of the data for\n",
    "# this simple model. As computed above by the product of Gaussian probability\n",
    "# density values.\n",
    "print(' ELBO loss : {} ,\\n - log prob of data : {}'.format(loss, -log_prob))\n",
    "\n",
    "# The training does not adjust the model since the model has no adjustable parameters\n",
    "plt.figure(1, dpi = 300)\n",
    "plt.plot(loss_sequence)\n",
    "plt.yscale(\"log\")\n",
    "plt.title('ELBO loss during training (log scale)')\n",
    "plt.xlabel('Epoch nr')\n",
    "plt.ylabel('value')\n",
    "\n",
    "\n",
    "# ii) Compare model output and data\n",
    "\n",
    "# Create the figure and 1x5 subplot grid\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "# Global y-axis limits\n",
    "y_min = T_meas.min()\n",
    "y_max = T_meas.max()\n",
    "\n",
    "# First plot: measurement data\n",
    "for i in range(n_device):\n",
    "    axes[0].scatter(T_true[i,:], T_meas[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0].set_title(\"Measurement data\")\n",
    "axes[0].set_xlabel(\"T_true\")\n",
    "axes[0].set_ylabel(\"T_meas\")\n",
    "axes[0].set_ylim(y_min, y_max)\n",
    "\n",
    "# Second plot: data produced by model pre-training\n",
    "for i in range(n_device):\n",
    "    axes[1].scatter(T_true[i,:], model_data_pretraining[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1].set_title(\"Data sampled from model pre-training\")\n",
    "axes[1].set_xlabel(\"T_true\")\n",
    "axes[1].set_ylabel(\"T_meas\")\n",
    "axes[1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by model post-training\n",
    "for i in range(n_device):\n",
    "    axes[2].scatter(T_true[i,:], model_data_posttraining[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[2].set_title(\"Data sampled from model post-training\")\n",
    "axes[2].set_xlabel(\"T_true\")\n",
    "axes[2].set_ylabel(\"T_meas\")\n",
    "axes[2].set_ylim(y_min, y_max)\n",
    "axes[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) Illustrate residuals\n",
    "\n",
    "# compute residuals\n",
    "residuals_data_pretrain = (T_true - T_meas).detach().numpy()\n",
    "residuals_data_posttrain = (alpha_0 + alpha_1 * T_true - T_meas ).detach().numpy()\n",
    "\n",
    "# Residual histograms for model pre- and post-training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), dpi=120)\n",
    "\n",
    "axes[0].hist(residuals_data_pretrain.flatten(), bins=30, edgecolor='black')\n",
    "axes[0].set_title(\"Residuals: Pre-trained model - measured\")\n",
    "axes[0].set_xlabel(\"Residual\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].hist(residuals_data_posttrain.flatten(), bins=30, edgecolor='black')\n",
    "axes[1].set_title(\"Residuals: Post-trained model - measured\")\n",
    "axes[1].set_xlabel(\"Residual\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßæ Summary\n",
    "\n",
    "‚úÖ Introduced `pyro.param` for deterministic unknowns  \n",
    "‚úÖ Used SVI to recover $(\\alpha_0,\\alpha_1)$ ‚âà least-squares solution  \n",
    "‚úÖ Verified match between Pyro and analytical regression  \n",
    "‚úÖ Visualised improved fit via ELBO, scatter plots, and residuals  \n",
    "\n",
    "Next up ‚Üí latent variables & uncertainty!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
