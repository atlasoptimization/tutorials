{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro Crash Course – Notebook 4: Latent Variables and Posterior Inference\n",
    "\n",
    "This notebook introduces **latent random variables** into our sensor calibration model — enabling a **fully Bayesian** approach with **posterior uncertainty**.\n",
    "\n",
    "### 🔙 Quick Recap\n",
    "\n",
    "*Notebook 3* used deterministic trainable parameters (`alpha_0`, `alpha_1`) for bias and scale.  \n",
    "It could **fit a line**, but gave **no uncertainty** on the estimated parameters.\n",
    "\n",
    "Today we upgrade the model to treat those parameters as **latent variables**, equipped with **Gaussian priors** and inferred using **variational approximation**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Learning Goals\n",
    "\n",
    "- Replace `pyro.param` with **latent samples** from a prior using `pyro.sample`\n",
    "- Introduce a **nontrivial guide** (variational posterior) and learn its parameters\n",
    "- Understand the ELBO when latent variables are present:  \n",
    "  $$\\mathrm{ELBO} = \\mathbb{E}_{q(z)}[\\log p(x,z) - \\log q(z)]$$\n",
    "- Compare:\n",
    "  - Prior Predictive Distribution\n",
    "  - Posterior Distribution\n",
    "  - Posterior Predictive Distribution\n",
    "- Validate inference quality against **closed-form Bayesian update**\n",
    "\n",
    "---\n",
    "\n",
    "For this, do the following:\n",
    "   1. Imports and definitions  \n",
    "   2. Build model and guide  \n",
    "   3. Perform inference  \n",
    "   4. Interpretations and illustrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/atlasoptimization/tutorials/blob/master/pyro/pyro_crash_course/pyro_cc_4_model_2.ipynb\n",
    ")\n",
    "\n",
    "> ⚠️ You do *not* need to sign in with GitHub to run this notebook.  \n",
    "> Just click the Colab badge and start executing.\n",
    "\n",
    "📎 Input: `sensor_measurements.csv` (generated in `pyro_cc_1_hello_dataset`) \n",
    "\n",
    "📌 Next: `pyro_cc_5_model_3` replaces the two unobserved latents by a hierarchy of latents\n",
    "\n",
    "\n",
    "This notebook and series are created for educational purposes by  [Dr. Jemil Avers Butt, Atlas Optimization GmbH](https://www.atlasoptimization.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start of Notebook 4\n",
    "\n",
    "## 1. Imports and definitions\n",
    "\n",
    "We import:\n",
    "\n",
    "- `pandas` to read the dataset generated in the previous notebook\n",
    "- `pyro` and `torch` to build the model and run inference\n",
    "- `matplotlib` for plotting\n",
    "\n",
    "We also **infer the shape** of the dataset (`n_device`, `n_measure`) and load the tensors for true and measured temperatures.\n",
    "\n",
    "📌 This is the first notebook that **introduces latent variables into the model**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. Imports and definitions\n",
    "\"\"\"\n",
    "\n",
    "# i) Imports\n",
    "# If you run this in Colab, you might get an import error as pyro is not\n",
    "# installed by default. In that case, uncomment the following command.\n",
    "# !pip install pyro-ppl\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "import pandas\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ii) Definitions\n",
    "# Read csv, infer dimensions\n",
    "# !wget https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
    "measurement_df = pandas.read_csv(\"sensor_measurements.csv\")\n",
    "n_device = measurement_df[\"sensor_id\"].nunique()\n",
    "n_measure = measurement_df[\"time_step\"].nunique()\n",
    "\n",
    "# Read out T_true and T_meas: the true temperature and the measured temperature\n",
    "T_true = torch.tensor(measurement_df[\"T_true\"].values).reshape(n_device, n_measure)\n",
    "T_meas = torch.tensor(measurement_df[\"T_measured\"].values).reshape(n_device, n_measure)\n",
    "\n",
    "# Assume standard deviation\n",
    "sigma_T_meas = 0.3\n",
    "\n",
    "# Fix random seed\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Build Model and Guide\n",
    "\n",
    "We now move from deterministic parameters to **latent random variables**.  \n",
    "This is our first *fully Bayesian* model in the series.\n",
    "\n",
    "### 🧱 Model Structure\n",
    "\n",
    "We assume each thermistor produces measurements according to:\n",
    "\n",
    "$$\n",
    "T_{\\text{measured}} = \\alpha_0 + \\alpha_1 \\cdot T_{\\text{true}} + \\text{Noise},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\alpha_0 \\sim \\mathcal{N}(0, 0.1)$ is an unknown offset\n",
    "- $\\alpha_1 \\sim \\mathcal{N}(1, 0.1)$ is an unknown scale\n",
    "- $\\text{Noise} \\sim \\mathcal{N}(0, \\sigma)$ is i.i.d. Gaussian noise with known $\\sigma = 0.3$\n",
    "\n",
    "Both $\\alpha_0$ and $\\alpha_1$ are **latent** – we assume prior knowledge but no direct observation. We will often use the symbol $z$ as representative for the latents, i.e. $z = (\\alpha_0, \\alpha_1)$ as $z$ is the standard notation to designate latents.\n",
    "\n",
    "This change allows:\n",
    "\n",
    "- Injection of prior knowledge (e.g. from spec sheets or calibration)\n",
    "- Estimation of **posterior distributions** over model parameters\n",
    "- Quantification of uncertainty through **posterior variance**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2. Build model and guide\n",
    "\"\"\"\n",
    "\n",
    "# i) Define the model\n",
    "\n",
    "# Define priors\n",
    "mu_alpha_0 = 0\n",
    "mu_alpha_1 = 1\n",
    "sigma_alpha_0 = 0.1\n",
    "sigma_alpha_1 = 0.1\n",
    "\n",
    "# Build the model in pyro\n",
    "def model(input_vars = T_true, observations = None):\n",
    "    # We dont need pyro.param anymore but instead we sample offset and scale from\n",
    "    # the prior distributions. This will declare them as random and require us\n",
    "    # to specify a variational distribution later in the guide.\n",
    "    alpha_0_prior = pyro.distributions.Normal(mu_alpha_0, sigma_alpha_0)\n",
    "    alpha_0 = pyro.sample('alpha_0', alpha_0_prior)\n",
    "    alpha_1_prior = pyro.distributions.Normal(mu_alpha_1, sigma_alpha_1)\n",
    "    alpha_1 = pyro.sample('alpha_1', alpha_1_prior)\n",
    "    \n",
    "    # Build the observation distribution using the sampled alpha_0 and alpha_1.\n",
    "    obs_dist = pyro.distributions.Normal(loc = alpha_0 + alpha_1 * T_true,\n",
    "                                         scale = sigma_T_meas)\n",
    "    \n",
    "    # Sample from this distribution and declare the samples independent in the\n",
    "    # first two dims. Independence here means that sampling the noise is done \n",
    "    # independently of device or T_true. It does not mean that there are independent\n",
    "    # alpha_0, alpha_1 for each device and measurement..\n",
    "    with pyro.plate('device_plate', dim = -2):\n",
    "        with pyro.plate('measure_plate', dim = -1):\n",
    "            obs = pyro.sample('observations', obs_dist, obs = observations)\n",
    "    \n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🤖 Guide: Inference Model\n",
    "\n",
    "To infer the unobserved values of $\\alpha_0, \\alpha_1$, we define a **guide**, i.e. a *variational approximation* to the posterior:\n",
    "\n",
    "$$\n",
    "q_\\phi(\\alpha_0, \\alpha_1) \\approx p(\\alpha_0, \\alpha_1 \\mid \\text{data})\n",
    "$$\n",
    "\n",
    "We choose:\n",
    "\n",
    "- Independent Normal distributions\n",
    "- Trainable parameters $\\mu_{\\phi}, \\sigma_{\\phi}$\n",
    "- Positivity constraints on the standard deviations\n",
    "\n",
    "This is the **first time in the course** where the guide contains learnable parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Modeling Insight\n",
    "\n",
    "Swapping deterministic values for latent variables moves us into Bayesian territory.  \n",
    "In contrast to `model_1`, where we fit best-point estimates (via MLE), we now:\n",
    "\n",
    "- Infer *distributions* over $\\alpha_0$, $\\alpha_1$\n",
    "- Capture uncertainty in those values\n",
    "- Enable sampling-based predictions with uncertainty bands\n",
    "\n",
    "While we shouldn't expect a performance boost (we're not modeling the structure any better), we now gain **epistemic uncertainty** over our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Build the guide\n",
    "def guide(input_vars = T_true, observations = None):\n",
    "    # We will approximate the posterior distributions of alpha_0, alpha_1 by\n",
    "    # Normal distributions with unknown means and standard deviations.\n",
    "    alpha_0_mu_post = pyro.param('alpha_0_mu_post', init_tensor = torch.zeros([]))\n",
    "    alpha_1_mu_post = pyro.param('alpha_1_mu_post', init_tensor = torch.ones([]))\n",
    "    \n",
    "    alpha_0_sigma_post = pyro.param('alpha_0_sigma_post', init_tensor = torch.ones([]),\n",
    "                                    constraint = pyro.distributions.constraints.positive)\n",
    "    alpha_1_sigma_post = pyro.param('alpha_1_sigma_post', init_tensor = torch.ones([]),\n",
    "                                    constraint = pyro.distributions.constraints.positive)\n",
    "\n",
    "    # In the guide we need to sample the unobserved latents to declare the model\n",
    "    # for their posteriors. \n",
    "    alpha_0_dist_post = pyro.distributions.Normal(loc = alpha_0_mu_post,\n",
    "                                                  scale = alpha_0_sigma_post)\n",
    "    alpha_1_dist_post = pyro.distributions.Normal(loc = alpha_1_mu_post,\n",
    "                                                  scale = alpha_1_sigma_post)\n",
    "\n",
    "    alpha_0 = pyro.sample('alpha_0', alpha_0_dist_post)\n",
    "    alpha_1 = pyro.sample('alpha_1', alpha_1_dist_post)\n",
    "\n",
    "    return alpha_0, alpha_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Illustration of model and guide\n",
    "\n",
    "To better visualize the beliefs of our model on which data are likely and how the latent variables are distributed, we can showcase again the graphical models. Note how the visual representation includes ellipses for $\\alpha_0, \\alpha_1$ because these are not simple parameters anymore. This time, also the guide is nontrivial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) Illustrate model and guide\n",
    "graphical_model = pyro.render_model(model = model, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "graphical_guide = pyro.render_model(model = guide, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"339pt\" height=\"203pt\"\n",
       " viewBox=\"0.00 0.00 339.35 203.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 199)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-199 335.3466,-199 335.3466,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_device_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"14.3466,-8 14.3466,-122 156.3466,-122 156.3466,-8 14.3466,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.8466\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">device_plate</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_measure_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"22.3466,-39 22.3466,-114 148.3466,-114 148.3466,-39 22.3466,-39\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.8466\" y=\"-46.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">measure_plate</text>\n",
       "</g>\n",
       "<!-- alpha_0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>alpha_0</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"38.3466\" cy=\"-168.5\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"38.3466\" y=\"-164.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0</text>\n",
       "</g>\n",
       "<!-- observations -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>observations</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"85.3466\" cy=\"-88\" rx=\"55.4913\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"85.3466\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">observations</text>\n",
       "</g>\n",
       "<!-- alpha_0&#45;&gt;observations -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>alpha_0&#45;&gt;observations</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M48.5449,-151.0327C54.784,-140.3466 62.8883,-126.4658 69.903,-114.4512\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"72.9864,-116.1118 75.0059,-105.7112 66.9413,-112.5823 72.9864,-116.1118\"/>\n",
       "</g>\n",
       "<!-- alpha_1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>alpha_1</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"133.3466\" cy=\"-168.5\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.3466\" y=\"-164.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1</text>\n",
       "</g>\n",
       "<!-- alpha_1&#45;&gt;observations -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>alpha_1&#45;&gt;observations</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M122.9313,-151.0327C116.5595,-140.3466 108.2827,-126.4658 101.1188,-114.4512\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.0349,-112.5077 95.9073,-105.7112 98.0226,-116.0927 104.0349,-112.5077\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"197.3466\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0 ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"197.3466\" y=\"-164.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1 ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"197.3466\" y=\"-149.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">observations ~ Normal</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fbd685a2850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"814pt\" height=\"178pt\"\n",
       " viewBox=\"0.00 0.00 814.00 178.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 174)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-174 810,-174 810,4 -4,4\"/>\n",
       "<!-- alpha_0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>alpha_0</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"118\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0</text>\n",
       "</g>\n",
       "<!-- alpha_1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>alpha_1</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"363\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"363\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1</text>\n",
       "</g>\n",
       "<!-- alpha_0_sigma_post -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>alpha_0_sigma_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"112,-128.5 0,-128.5 0,-113.5 112,-113.5 112,-128.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-117.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0_sigma_post</text>\n",
       "</g>\n",
       "<!-- alpha_0_sigma_post&#45;&gt;alpha_0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>alpha_0_sigma_post&#45;&gt;alpha_0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M60.6601,-113.2583C69.3616,-98.8025 88.4435,-67.1019 102.208,-44.2352\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.213,-46.0296 107.3716,-35.6569 99.2157,-42.4195 105.213,-46.0296\"/>\n",
       "</g>\n",
       "<!-- alpha_1_sigma_post -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>alpha_1_sigma_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"358,-128.5 246,-128.5 246,-113.5 358,-113.5 358,-128.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"302\" y=\"-117.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1_sigma_post</text>\n",
       "</g>\n",
       "<!-- alpha_1_sigma_post&#45;&gt;alpha_1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>alpha_1_sigma_post&#45;&gt;alpha_1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M306.5849,-113.2583C315.1082,-98.8664 333.7543,-67.3821 347.2826,-44.5392\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"350.4587,-46.0447 352.543,-35.6569 344.4357,-42.4777 350.4587,-46.0447\"/>\n",
       "</g>\n",
       "<!-- alpha_0_mu_post -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>alpha_0_mu_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"227.5,-128.5 130.5,-128.5 130.5,-113.5 227.5,-113.5 227.5,-128.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"179\" y=\"-117.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0_mu_post</text>\n",
       "</g>\n",
       "<!-- alpha_0_mu_post&#45;&gt;alpha_0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>alpha_0_mu_post&#45;&gt;alpha_0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M174.4151,-113.2583C165.8918,-98.8664 147.2457,-67.3821 133.7174,-44.5392\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.5643,-42.4777 128.457,-35.6569 130.5413,-46.0447 136.5643,-42.4777\"/>\n",
       "</g>\n",
       "<!-- alpha_1_mu_post -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>alpha_1_mu_post</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"473.5,-128.5 376.5,-128.5 376.5,-113.5 473.5,-113.5 473.5,-128.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"425\" y=\"-117.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1_mu_post</text>\n",
       "</g>\n",
       "<!-- alpha_1_mu_post&#45;&gt;alpha_1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>alpha_1_mu_post&#45;&gt;alpha_1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M420.3399,-113.2583C411.6384,-98.8025 392.5565,-67.1019 378.792,-44.2352\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"381.7843,-42.4195 373.6284,-35.6569 375.787,-46.0296 381.7843,-42.4195\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"500\" y=\"-154.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0 ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"500\" y=\"-139.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1 ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"500\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0_mu_post : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"500\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1_mu_post : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"500\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_0_sigma_post : GreaterThan(lower_bound=0.0)</text>\n",
       "<text text-anchor=\"start\" x=\"500\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha_1_sigma_post : GreaterThan(lower_bound=0.0)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fbd6855b2b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphical_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the output of sampling from prior-predictive, posterior, and posterior-predictive distributions.\n",
    "\n",
    "| Symbol | Definition | Pyro call |\n",
    "|--------|------------|-----------|\n",
    "| **Prior-predictive**  $p_\\theta(x,z)$ | Draw latents $z$ from the prior, then simulate data $x$ from the likelihood | `Predictive(model)` |\n",
    "| **Posterior**  $q_\\phi(z)\\!\\approx\\!p_\\theta(z\\mid x)$ | Variational approximation to true posterior | `Predictive(guide)` |\n",
    "| **Posterior-predictive**  $p_\\theta(x' \\mid x)$ | Draw $z\\!\\sim\\!q_\\phi$ and simulate new data $x'$ | `Predictive(model, guide=guide)` |\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_\\theta(x,z) &= p_\\theta(z)\\,p_\\theta(x\\mid z),\\\\[4pt]\n",
    "q_\\phi(z) &\\approx p_\\theta(z\\mid x),\\\\[4pt]\n",
    "p_\\theta(x' \\mid x) &= \\int p_\\theta(x' \\mid z)\\,q_\\phi(z)\\,dz.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Sampling each of these gives us intuition about **what the model believes** before and after observing the data. We will plot these results later in section 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) Record example outputs of model and guide prior to training\n",
    "n_model_samples = 30\n",
    "n_guide_samples = 1000  \n",
    "\n",
    "predictive = pyro.infer.Predictive\n",
    "prior_predictive_pretrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_pretrain_dict = predictive(guide, num_samples = n_guide_samples)()\n",
    "posterior_pretrain = torch.vstack((posterior_pretrain_dict['alpha_0'],\n",
    "                                   posterior_pretrain_dict['alpha_1'])).T\n",
    "posterior_predictive_pretrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Perform Inference\n",
    "\n",
    "We use **Stochastic Variational Inference** (SVI) to optimize the guide parameters.\n",
    "\n",
    "This involves:\n",
    "\n",
    "1. Sampling latents $z$ from the guide  \n",
    "2. Plugging sampled latents $z$ into the model  \n",
    "3. Estimating the ELBO  \n",
    "4. Taking gradients and updating parameters\n",
    "\n",
    "We increase `num_particles` to 5 to reduce variance in the Monte Carlo ELBO estimate.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔬 ELBO with Latent Variables\n",
    "\n",
    "The Evidence Lower Bound (ELBO) becomes more complex in this model:\n",
    "\n",
    "$$\n",
    "\\text{ELBO}(\\theta, \\phi) = \n",
    "\\underbrace{\\mathbb{E}_{q_\\phi}[\\log p_\\theta(x, z)]}_{\\text{joint likelihood}} - \n",
    "\\underbrace{\\mathbb{E}_{q_\\phi}[\\log q_\\phi(z)]}_{\\text{posterior entropy}}\n",
    "$$\n",
    "\n",
    "Or, in the equivalent KL-divergence form:\n",
    "\n",
    "$$\n",
    "\\text{ELBO} = \n",
    "- \\log p_\\theta(x) + \n",
    "\\text{KL}\\big(q_\\phi(z) \\;||\\; p_\\theta(z \\mid x)\\big)\n",
    "$$\n",
    "\n",
    "🧠 Key Observations:\n",
    "\n",
    "- **We cannot compute $\\log p_\\theta(x)$** directly – we estimate it via samples.\n",
    "- The **KL divergence** no longer vanishes (as it did in previous models).\n",
    "- ELBO becomes a **stochastic objective**, with noise coming from both latent and noise sampling.\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 Consequences for Training\n",
    "\n",
    "- The ELBO curve will be **noisy**\n",
    "- You can't directly compare ELBO values across models anymore\n",
    "- The ELBO is now *strictly above* the negative log-likelihood, due to the KL term\n",
    "\n",
    "Despite these challenges, this shift to **variational Bayesian inference** allows more flexible modeling and better uncertainty quantification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    3. Perform inference\n",
    "\"\"\"\n",
    "\n",
    "# i) Set up inference\n",
    "adam = pyro.optim.Adam({\"lr\": 1.0})\n",
    "elbo = pyro.infer.Trace_ELBO(num_particles = 5)\n",
    "svi = pyro.infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "# ii) Perform svi\n",
    "data = (T_true, T_meas)\n",
    "loss_sequence = []\n",
    "for step in range(1000):\n",
    "    loss = svi.step(*data)\n",
    "    loss_sequence.append(loss)\n",
    "    if step %50 == 0:\n",
    "        print(f'epoch: {step} ; loss : {loss}')        \n",
    "\n",
    "# iii) Record example outputs of model and guide after training\n",
    "prior_predictive_posttrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_posttrain_dict = predictive(guide, num_samples = n_guide_samples)()\n",
    "posterior_posttrain = torch.vstack((posterior_posttrain_dict['alpha_0'],\n",
    "                                   posterior_posttrain_dict['alpha_1'])).T\n",
    "posterior_predictive_posttrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n",
    "\n",
    "\n",
    "# iv) Additional investigations\n",
    "# We now inspect the model and guide using pyros inbuilt functionality. This allows\n",
    "# us insight into interior details of model and guide; but we only plot the shapes.\n",
    "model_trace = pyro.poutine.trace(model).get_trace(T_true, observations = T_meas)\n",
    "guide_trace = pyro.poutine.trace(guide).get_trace(T_true, observations = T_meas)\n",
    "print('These are the shapes inside of the model : \\n {}'.format(model_trace.format_shapes()))\n",
    "print('These are the shapes inside of the guide : \\n {}'.format(guide_trace.format_shapes()))\n",
    "\n",
    "# The parameters of the posterior are again stored in pyro's param_store\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print('Param : {}; Value : {}'.format(name, value))\n",
    "   \n",
    "# Note: the log_prob values are also accessible by inspection of the model trace.\n",
    "# But now they do not exactly correspond to the ELBO.\n",
    "model_trace.compute_log_prob()\n",
    "log_prob_from_trace = torch.sum(model_trace.nodes[\"observations\"][\"log_prob\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🧮 Closed-Form Posterior Comparison\n",
    "\n",
    "For this model, we can derive the true posterior analytically.\n",
    "\n",
    "We compute:\n",
    "\n",
    "- The **posterior mean and covariance** from conditioning a multivariate Gaussian\n",
    "- The result is exact under the model assumptions\n",
    "\n",
    "Then we compare:\n",
    "\n",
    "- Posterior means from Pyro (via `pyro.param`)\n",
    "- Posterior means from the closed-form solution\n",
    "\n",
    "✅ The means match well  \n",
    "⚠️ The variances from SVI are **underestimated**\n",
    "\n",
    "> This is a known issue with **mean-field variational inference**, which cannot capture posterior correlations.  \n",
    "See *Blei et al., \"Variational Inference: A Review for Statisticians\"*, page 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the closed form solution for this problem by deriving the posterior\n",
    "A = torch.vstack((torch.ones([n_device*n_measure]), T_true.flatten())).T\n",
    "mu_alpha = torch.tensor([mu_alpha_0, mu_alpha_1], dtype = torch.float64)\n",
    "Sigma_alpha = torch.tensor([[sigma_alpha_0**2, 0], [0, sigma_alpha_1**2]], dtype = torch.float64)\n",
    "Sigma_noise = (sigma_T_meas**2)*torch.eye(n_device*n_measure)\n",
    "Sigma_y = A@Sigma_alpha@A.T + Sigma_noise\n",
    "\n",
    "mu_alpha_cf = mu_alpha + Sigma_alpha @ A.T@torch.linalg.pinv((A@Sigma_alpha@A.T\n",
    "                        +  Sigma_noise)) @(T_meas.flatten() - T_true.flatten())\n",
    "Sigma_alpha_cf = Sigma_alpha - Sigma_alpha @ A.T@torch.linalg.pinv((A@Sigma_alpha@A.T\n",
    "                        + Sigma_noise)) @A@Sigma_alpha\n",
    "\n",
    "print('The closed form solution is \\n alpha_0_mu_post = {}, alpha_0_sigma_post = {};\\n'\n",
    "      'alpha_1_mu_post = {}, alpha_1_sigma_post = {};\\n'\n",
    "      .format(mu_alpha_cf[0], torch.sqrt(Sigma_alpha_cf[0,0]), \n",
    "              mu_alpha_cf[1], torch.sqrt(Sigma_alpha_cf[1,1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Interpretations and Illustrations\n",
    "\n",
    "### 📉 ELBO Loss During Training\n",
    "\n",
    "We visualize the ELBO during optimization:\n",
    "\n",
    "- The loss is **noisy** due to stochastic sampling of latent variables and ELBO estimation\n",
    "- We plot it on a **log scale** to highlight multiplicative improvement\n",
    "\n",
    "This curve shows how inference gradually improves our approximation of the true posterior. We do **not** expect a perfectly smooth descent.\n",
    "\n",
    "\n",
    "\n",
    "### 📊 Predictive and Posterior Visualizations\n",
    "\n",
    "We now compare the following distributions before and after training:\n",
    "\n",
    "| Distribution Type               | Description                                                                 |\n",
    "|----------------------------------|-----------------------------------------------------------------------------|\n",
    "| **Prior predictive**             | Model with $\\alpha_0, \\alpha_1$ sampled from priors                         |\n",
    "| **Posterior predictive**         | Model conditioned on latent samples from the guide                         |\n",
    "| **Guide posterior (2D histogram)**| Samples from $q_\\phi(\\alpha_0, \\alpha_1)$                                  |\n",
    "\n",
    "We plot:\n",
    "\n",
    "1. Measurement data\n",
    "2. Samples from the prior predictive\n",
    "3. Samples from the posterior predictive\n",
    "4. The learned posterior over $(\\alpha_0, \\alpha_1)$\n",
    "\n",
    "➡ These views allow us to:\n",
    "\n",
    "- See how training collapses uncertainty\n",
    "- Understand how parameter distributions affect outputs\n",
    "- Compare data fidelity visually\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Insight: What We Gain\n",
    "\n",
    "Although this model has no more structure than `model_1`, it gives us something new:\n",
    "\n",
    "- **Epistemic uncertainty** about offset and scale\n",
    "- **Credible intervals** via posterior predictive sampling\n",
    "- A **Bayesian foundation** for future hierarchical models\n",
    "\n",
    "This sets the stage for `model_3`, where $\\alpha_0$, $\\alpha_1$ will be allowed to vary across devices with **hierarchical priors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    4. Interpretations and illustrations\n",
    "\"\"\"\n",
    "\n",
    "# i) Plot and print ELBO loss\n",
    "plt.figure(1, dpi = 300)\n",
    "plt.plot(loss_sequence)\n",
    "plt.yscale(\"log\")\n",
    "plt.title('ELBO loss during training (log scale)')\n",
    "plt.xlabel('Epoch nr')\n",
    "plt.ylabel('value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ii) Compare model output and data\n",
    "\n",
    "# Create the figure and 1x5 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=False, sharey=False)\n",
    "# Global y-axis limits\n",
    "y_min = T_meas.min()\n",
    "y_max = T_meas.max()\n",
    "\n",
    "# FIRST ROW\n",
    "# First plot: measurement data\n",
    "for i in range(n_device):\n",
    "    axes[0,0].scatter(T_true[i,:], T_meas[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,0].set_title(\"Measurement data\")\n",
    "axes[0,0].set_xlabel(\"T_true\")\n",
    "axes[0,0].set_ylabel(\"T_meas\")\n",
    "axes[0,0].set_ylim(y_min, y_max)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Second plot: data produced by model pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,1].scatter(T_true[i,:], prior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,1].set_title(\"Data from model pre-training\")\n",
    "axes[0,1].set_xlabel(\"T_true\")\n",
    "axes[0,1].set_ylabel(\"T_meas\")\n",
    "axes[0,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,2].scatter(T_true[i,:], posterior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,2].set_title(\"Data from posterior predictive pre-training\")\n",
    "axes[0,2].set_xlabel(\"T_true\")\n",
    "axes[0,2].set_ylabel(\"T_meas\")\n",
    "axes[0,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide pre-training\n",
    "axes[0,3].hist2d(posterior_pretrain[:,0].numpy(), posterior_pretrain[:,1].numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[0,3].set_title(\"2D Histogram of parameters pre-train\")\n",
    "axes[0,3].set_xlabel(\"alpha_0\")\n",
    "axes[0,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "# SECOND ROW\n",
    "# Second plot: data produced by model post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,1].scatter(T_true[i,:], prior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,1].set_title(\"Data from model post-training\")\n",
    "axes[1,1].set_xlabel(\"T_true\")\n",
    "axes[1,1].set_ylabel(\"T_meas\")\n",
    "axes[1,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,2].scatter(T_true[i,:], posterior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,2].set_title(\"Data from posterior predictive post-training\")\n",
    "axes[1,2].set_xlabel(\"T_true\")\n",
    "axes[1,2].set_ylabel(\"T_meas\")\n",
    "axes[1,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide post-training\n",
    "axes[1,3].hist2d(posterior_posttrain[:,0].numpy(), posterior_posttrain[:,1].numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[1,3].set_title(\"2D Histogram of parameters post-train\")\n",
    "axes[1,3].set_xlabel(\"alpha_0\")\n",
    "axes[1,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Note that here we illustrate multiple realizations of the model. We notice that\n",
    "# after training the band of values considered possible has been shrinked down\n",
    "# and especially the scale is known with much more certainty as assumed before.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧾 Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "✅ Defined a linear model with **latent parameters** (scale and offset)  \n",
    "✅ Used **priors** over those parameters and sampled them in the model  \n",
    "✅ Built a **nontrivial guide** to approximate their posterior  \n",
    "✅ Performed **Bayesian inference** using SVI with `num_particles > 1`  \n",
    "✅ Compared **prior predictive**, **posterior predictive**, and **posterior** samples  \n",
    "✅ Verified our results against a **closed-form posterior**\n",
    "\n",
    "---\n",
    "\n",
    "📌 In the next notebook, we'll introduce **hierarchical priors**, allowing each device to have its own scale and offset.  \n",
    "This lets us model **inter-device variation** and learn shared structure via **Bayesian pooling**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
