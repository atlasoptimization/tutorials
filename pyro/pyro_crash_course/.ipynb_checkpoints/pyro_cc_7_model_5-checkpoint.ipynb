{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro Crash Course ‚Äì Notebook 7: Learning Nonlinear Drift with Neural Nets\n",
    "\n",
    "This final notebook adds a learnable **nonlinear function** into our probabilistic model. We use an **artificial neural network (ANN)** to model a shared drift that affects all measurements, extending the expressive power of our calibration framework.\n",
    "\n",
    "---\n",
    "\n",
    "### üîô Quick Recap\n",
    "Notebook 6 allowed each sensor to be classified as faulty or functioning, introducing discrete latent variables and structured priors per class. \n",
    "\n",
    "Now we go a step further: we assume that all devices experience some unknown nonlinear drift, perhaps due to an environmental factor not captured in the model. We use a neural network to learn this drift directly from the data‚Äîwhile still performing joint inference over:\n",
    " - Per-device scale and offset\n",
    " - Device failure mode (is_faulty)\n",
    " - Shared nonlinear trends (ANN(T_true))\n",
    " \n",
    "---\n",
    "\n",
    "### üéØ Learning Goals\n",
    "\n",
    "- Add a neural network to a Pyro model using torch.nn and pyro.module\n",
    "- Treat ANN parameters as deterministic learnable components\n",
    "- Combine continuous, discrete, and functional unknowns in a single model\n",
    "- Interpret the learned ANN as a shared, nonlinear correction to the measured data\n",
    "- Train models that resemble real calibration setups with subtle unknown effects\n",
    "\n",
    "---\n",
    "\n",
    "### üõ† Modeling Idea\n",
    "\n",
    "We assume each measurement is generated as:\n",
    "\n",
    "$$\n",
    "T_{\\text{meas}} = \\alpha_0 + \\alpha_1 \\cdot T_{\\text{true}} + \\text{ANN}(T_{\\text{true}}) + \\varepsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha_0, \\alpha_1$ are **latent calibration parameters per device**\n",
    "- $\\text{ANN}(T_{\\text{true}})$ is a **shared nonlinear drift**\n",
    "- $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is i.i.d. measurement noise\n",
    "- Devices may be **faulty**, in which case their calibration parameters are drawn from broader priors\n",
    "\n",
    "We treat the ANN as a fixed function during inference, but let its parameters be **optimized via the ELBO**‚Äîjust like in classical machine learning.\n",
    "\n",
    "\n",
    "\n",
    "For this, do the following:\n",
    "   1. Imports and definitions  \n",
    "   2. Build model and guide  \n",
    "   3. Perform inference  \n",
    "   4. Interpretations and illustrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/atlasoptimization/tutorials/blob/master/pyro/pyro_crash_course/pyro_cc_7_model_5.ipynb\n",
    ")\n",
    "\n",
    "> ‚ö†Ô∏è You do *not* need to sign in with GitHub to run this notebook.  \n",
    "> Just click the Colab badge and start executing.\n",
    "\n",
    "üìé Input: `sensor_measurements.csv` (generated in `pyro_cc_1_hello_dataset`) \n",
    "\n",
    "üìå This is the **final notebook** of the pyro crash course!\n",
    "\n",
    "\n",
    "This notebook and series are created for educational purposes by  [Dr. Jemil Avers Butt, Atlas Optimization GmbH](https://www.atlasoptimization.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start of Notebook 7\n",
    "\n",
    "## 1. Imports and definitions\n",
    "\n",
    "No changes here. Just setup ‚Äî `pyro`, `torch`, CSV import, reshaping, and seed.\n",
    "\n",
    "\n",
    "üìå This is the first notebook that **introduces per-device latent variables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. Imports and definitions\n",
    "\"\"\"\n",
    "\n",
    "# i) Imports\n",
    "# If you run this in Colab, you might get an import error as pyro is not\n",
    "# installed by default. In that case, uncomment the following command.\n",
    "# !pip install pyro-ppl\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pyro.infer import config_enumerate\n",
    "\n",
    "\n",
    "# ii) Definitions\n",
    "!wget https://raw.githubusercontent.com/atlasoptimization/tutorials/master/pyro/pyro_crash_course/sensor_measurements.csv\n",
    "measurement_df = pandas.read_csv(\"sensor_measurements.csv\")\n",
    "n_device = measurement_df[\"sensor_id\"].nunique()\n",
    "n_measure = measurement_df[\"time_step\"].nunique()\n",
    "\n",
    "# Read out T_true and T_meas: the true temperature and the measured temperature\n",
    "T_true = torch.tensor(measurement_df[\"T_true\"].values).reshape(n_device, n_measure)\n",
    "T_meas = torch.tensor(measurement_df[\"T_measured\"].values).reshape(n_device, n_measure)\n",
    "\n",
    "# Assume standard deviation\n",
    "sigma_T_meas = 0.3\n",
    "\n",
    "# Fix random seed\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Build model and guide\n",
    "\n",
    "## üîß Model Specification\n",
    "\n",
    "We extend the hierarchical model from Notebook 6 by including a shared **nonlinear drift** term modeled by a neural network. This shared component captures global trends across all devices and is trained jointly with the latent variables.\n",
    "\n",
    "Each sensor is still either:\n",
    "\n",
    "- ‚úÖ **Functioning normally**: Parameters drawn from `ùí©([0, 1], Œ£)`\n",
    "- ‚ùå **Faulty**: Parameters drawn from `ùí©([0, 1], Œ£_faulty)` with much larger uncertainty\n",
    "\n",
    "As before, each device has a discrete latent variable:\n",
    "\n",
    "$$\n",
    "\\text{is_faulty}_i \\sim \\text{Bernoulli}(p_\\text{faulty})\n",
    "$$\n",
    "\n",
    "The covariance of the calibration prior depends on faultiness:\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{device},i} = \\Sigma_{\\text{normal}} + \\text{is_faulty}_i \\cdot (\\Sigma_{\\text{faulty}} - \\Sigma_{\\text{normal}})\n",
    "$$\n",
    "\n",
    "The calibration parameters (offset and scale) are sampled per device:\n",
    "\n",
    "$$\n",
    "\\alpha_i \\sim \\mathcal{N}(\\mu_\\alpha, \\Sigma_{\\text{device},i})\n",
    "\\quad \\text{where} \\quad\n",
    "\\mu_\\alpha = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Shared Nonlinear Drift\n",
    "\n",
    "On top of this per-device affine structure, we introduce a shared **neural network drift** `f(T_true)` that adjusts the predicted value globally:\n",
    "\n",
    "$$\n",
    "T_{\\text{meas}}[i,j] \\sim \\mathcal{N} \\left(\n",
    "    \\alpha_{i,0} + \\alpha_{i,1} \\cdot T_{\\text{true}}[i,j] + f(T_{\\text{true}}[i,j]),\n",
    "    \\sigma_\\text{Tmeas}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "The drift function `f` is implemented via a PyTorch neural network (`torch.nn.Sequential`) and registered with Pyro via `pyro.module(\"ann\", ann)` to ensure its parameters are optimized. Unlike the device-specific `Œ±`, which vary across sensors, the neural net learns a **global nonlinear correction** shared by all sensors. This allows Pyro to:\n",
    "\n",
    "- Capture smooth distortions in temperature behavior\n",
    "- Avoid hardcoding the structure of the drift\n",
    "- Automatically learn patterns not explainable by offsets or scale\n",
    "\n",
    "The drift function is fully differentiable and fits naturally into the SVI framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define the ANN\n",
    "\n",
    "In this notebook, we define a **small feedforward neural network** that learns a global nonlinear drift correction over all temperature measurements. It's used as an additive term in the model:\n",
    "\n",
    "$$\n",
    "T_{\\text{meas}} \\approx \\text{offset} + \\text{scale} \\cdot T_{\\text{true}} + \\text{ANN}(T_{\\text{true}})\n",
    "$$\n",
    "\n",
    "### üîß Architecture\n",
    "\n",
    "- **Input:** scalar `T_true[i,j] ‚àà ‚Ñù`  \n",
    "- **Output:** scalar drift adjustment for each `[i,j]`  \n",
    "- **Layers:**\n",
    "  - `Linear(1 ‚Üí 8)` + `Tanh`\n",
    "  - `Linear(8 ‚Üí 8)` + `Tanh`\n",
    "  - `Linear(8 ‚Üí 1)`\n",
    "\n",
    "### üîç Purpose\n",
    "\n",
    "- Captures **global nonlinear trends** shared across all sensors\n",
    "- Complements the **per-device linear calibration model**\n",
    "- Is treated as a deterministic function with trainable parameters, optimized via ELBO\n",
    "\n",
    "### ‚úÖ Notes\n",
    "\n",
    "- The network is built as a subclass of `torch.nn.Module`\n",
    "- Registered via `pyro.module(\"ann\", ann)` to ensure parameters are included in optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) Define the ANN)\n",
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize instance using init method from base class\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create linear and nonlinear transforms\n",
    "        self.lin_1 = torch.nn.Linear(1,8)\n",
    "        self.lin_2 = torch.nn.Linear(8,8)\n",
    "        self.lin_3 = torch.nn.Linear(8,1)\n",
    "        self.nonlinear = torch.nn.Tanh()\n",
    "    def forward(self, t):\n",
    "        # Define forward computation on the input data T_true.\n",
    "        t = t.reshape([-1, 1])\n",
    "        hidden_units_1 = self.nonlinear(self.lin_1(t))\n",
    "        hidden_units_2 = self.nonlinear(self.lin_2(hidden_units_1))\n",
    "        nonlinear_drift = self.lin_3(hidden_units_2)\n",
    "        \n",
    "        nonlinear_drift = nonlinear_drift.reshape([n_device, n_measure])\n",
    "        return nonlinear_drift\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Neural Networks in Pyro\n",
    "\n",
    "Neural networks can serve as powerful nonlinear function approximators inside probabilistic models. In Pyro, you can treat them as deterministic modules with learnable parameters ‚Äî useful for modeling **unknown systematics**, **sensor drift**, **nonlinear response functions**, and more.\n",
    "\n",
    "### üß© Use Case\n",
    "\n",
    "Suppose we model an observation $y$ as:\n",
    "\n",
    "$$\n",
    "y  \\approx \\text{ANN}(x) + \\varepsilon\n",
    "$$\n",
    "\n",
    "Here, the neural network learns a data-driven **nonlinear mapping** to map $x$ to $y$. We showcase below an example of how ANN's are built in pyro and then integrated into pyro models. Typically, this involves three steps:\n",
    "\n",
    "- Define a class ANN and the layers to be used (subclass torch.nn.Module and use torch.nn differentiable layers)\n",
    "- Define the ANN classes forward() method by chaining the layers (= forward pass of the ANN)\n",
    "- Invoke an instance of ANN and register it inside of the model via pyro.module() (= mark params for optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Minimal Pyro Pattern\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "# 1. Define ANN (standard torch.nn API works fine)\n",
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize instance using init method from base class\n",
    "        super().__init__()\n",
    "        \n",
    "        # transforms: linear, nonlinear\n",
    "        self.lin_1 = torch.nn.Linear(1,64)\n",
    "        self.lin_2 = torch.nn.Linear(64,1)\n",
    "        self.nonlinear = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape, then define how to chain the operations on input x\n",
    "        x = x.reshape([-1, 1])\n",
    "        hidden_units = self.nonlinear(self.lin_1(x))\n",
    "        nonlinear_drift = self.lin_2(hidden_units)\n",
    "        return nonlinear_drift\n",
    "\n",
    "# 2. Instantiate, then use as normal differentiable function\n",
    "ann = ANN()\n",
    "x_example = torch.linspace(0,1,100)\n",
    "y = ann(x_example)\n",
    "\n",
    "def model(x, y_obs=None):\n",
    "    pyro.module(\"ann\", ann)  # <- register all trainable params\n",
    "    # ... rest of model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and guide\n",
    "\n",
    "The subsequent code for model and guide are essentially identical to the code provided in the previous notebook. The only differences are marking the params inside of the ann for optimization (via `pyro.module(\"ann\", ann)`) and adding the neural network term to the mean (via `mean_obs = ... + ann(input_vars`)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Define the model\n",
    "\n",
    "# Define priors and fixed params\n",
    "mu_alpha = torch.tensor([0.0, 1.0]).expand(n_device, -1)\n",
    "Sigma_alpha = torch.tensor([[0.1**2,0], [0, 0.1**2]])\n",
    "p_faulty = 0.05\n",
    "Sigma_faulty = 100*Sigma_alpha\n",
    "\n",
    "# Now instantiate the neural net ann; set the params to double to match the data\n",
    "ann = ANN().double()\n",
    "\n",
    "# Build the model in pyro; just mix in ann() like any normal torch function\n",
    "@config_enumerate\n",
    "def model(input_vars = T_true, observations = None):\n",
    "    # Mark the parameters inside of the ann for optimization\n",
    "    pyro.module(\"ann\", ann)\n",
    "    \n",
    "    # Build reusable independence context device_plate and sample\n",
    "    device_plate = pyro.plate('device_plate', size=n_device, dim=-1)\n",
    "    with device_plate:\n",
    "        is_faulty = pyro.sample('is_faulty', pyro.distributions.Bernoulli(p_faulty))\n",
    "    is_faulty_tensor = is_faulty.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    # Build alpha_dist and sample\n",
    "    Sigma_device = Sigma_alpha + is_faulty_tensor * (Sigma_faulty - Sigma_alpha)  # shape: [n_device, 2, 2]\n",
    "    alpha_dist = pyro.distributions.MultivariateNormal(\n",
    "        loc=mu_alpha,                   # shape: [2, n_device, 2]\n",
    "        covariance_matrix=Sigma_device)     # shape: [2, n_device, 2, 2]\n",
    "    with device_plate:\n",
    "        alpha = pyro.sample(\"alpha\", alpha_dist)\n",
    "        \n",
    "    # Build observation dist and sample\n",
    "    mean_obs = alpha[:, 0].unsqueeze(1) + alpha[:, 1].unsqueeze(1) * T_true + ann(input_vars)\n",
    "    obs_dist = pyro.distributions.Normal(loc=mean_obs, scale=sigma_T_meas)\n",
    "    with pyro.plate('device_plate', dim=-2):\n",
    "        with pyro.plate('measure_plate', dim=-1):\n",
    "            pyro.sample(\"observations\", obs_dist, obs=observations)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ii) Build the guide\n",
    "# Build the guide\n",
    "@config_enumerate\n",
    "def guide(input_vars = T_true, observations = None):\n",
    "    # Build reusable independence context device_plate.\n",
    "    device_plate = pyro.plate('device_plate', size=n_device, dim=-1)\n",
    "    \n",
    "    # Learn a per-device probability of being faulty\n",
    "    probs_faulty = pyro.param(\"probs_faulty\", 0.05 * torch.ones(n_device),\n",
    "                              constraint=pyro.distributions.constraints.unit_interval)\n",
    "    \n",
    "    # Sample discrete variable from learned probabilities\n",
    "    with device_plate:\n",
    "        is_faulty = pyro.sample(\"is_faulty\", pyro.distributions.Bernoulli(probs_faulty))\n",
    "    \n",
    "    # Per-device means and scales (shape [n_device, 2])\n",
    "    mu_alpha_post = pyro.param('mu_alpha_post', torch.tensor([0,1.0]).unsqueeze(0).expand([n_device,2]))\n",
    "    sigma_alpha_post = (pyro.param('sigma_alpha_post', 0.1 * (torch.eye(2).unsqueeze(0)).expand([n_device,2,2]),\n",
    "                              constraint=pyro.distributions.constraints.positive_definite) \n",
    "                        + 0.001 * torch.eye(2))\n",
    "    # We add 1e-3 on the diagonal of the covariance matrix to avoid numerical issues\n",
    "    # related to positive definiteness tests.\n",
    "\n",
    "    with device_plate:\n",
    "        # Multivariate Gaussian for each device (allow for correlations)\n",
    "        alpha = pyro.sample('alpha', pyro.distributions.MultivariateNormal(loc = mu_alpha_post,\n",
    "                                        covariance_matrix = sigma_alpha_post))\n",
    "\n",
    "    return alpha, is_faulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iii) Illustrate model and guide\n",
    "\n",
    "# Now the illustration of the model shows additional parameters representing the\n",
    "# biases and weights of the linear layers in the neural network. The guide looks\n",
    "# the same as before.\n",
    "graphical_model = pyro.render_model(model = model, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "graphical_guide = pyro.render_model(model = guide, model_args= (T_true,),\n",
    "                                    render_distributions=True,\n",
    "                                    render_params=True)\n",
    "\n",
    "graphical_model\n",
    "graphical_guide\n",
    "\n",
    "\n",
    "# iv) Record example outputs of model and guide prior to training\n",
    "\n",
    "n_model_samples = 10\n",
    "n_guide_samples = 1000  \n",
    "\n",
    "predictive = pyro.infer.Predictive\n",
    "prior_predictive_pretrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_pretrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_pretrain_faulty = predictive(guide, num_samples = n_guide_samples)()['is_faulty']\n",
    "posterior_predictive_pretrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "    3. Perform inference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# i) Set up inference\n",
    "\n",
    "# In terms of loss function, we need now TraceEnum_ELBO instead of Trace_ELBO,\n",
    "# as the latter one is not designed to handle enumeration.\n",
    "adam = pyro.optim.Adam({\"lr\": 0.01})\n",
    "elbo = pyro.infer.TraceEnum_ELBO(num_particles = 10,\n",
    "                                 max_plate_nesting = 2)\n",
    "svi = pyro.infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "\n",
    "# ii) Perform svi\n",
    "\n",
    "data = (T_true, T_meas)\n",
    "loss_sequence = []\n",
    "grad_sequence = []\n",
    "for step in range(5000):\n",
    "    loss = svi.step(*data)\n",
    "    loss_sequence.append(loss)\n",
    "    if step %50 == 0:\n",
    "        print(f'epoch: {step} ; loss : {loss}')\n",
    "        grad_sequence.append(ann.lin_1.weight.grad[0])\n",
    "          \n",
    "# We run this inference for a longer time than for the other models. Learning \n",
    "# the nonlinear drift turns out to be a bit difficult after all! The training\n",
    "# procedure is prone to just learning model_4 and ignoring the nonlinear trend\n",
    "# by setting it to 0 or some basically linear unexpressive function. To actually\n",
    "# recover parts of the sinusoidal trend, we increase the randomness of the inference\n",
    "# by increading the number of steps and decreasing the number of particles. That\n",
    "# allows better exploration of the model space and taking us out of local minima\n",
    "# As a downside, running this cell might take 20 mins this is the right point in\n",
    "# time to already skip ahead with reading the summary and condensing the whole\n",
    "# of the crash course for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Troubleshooting Checklist\n",
    "\n",
    "| Symptom                      | Likely Cause                        | Fix                                                          |\n",
    "| ---------------------------- | ----------------------------------- | ------------------------------------------------------------ |\n",
    "| Parameters don't change      | `pyro.module()` is missing          | Add `pyro.module(\"ann\", ann)` inside `model()`               |\n",
    "| Gradients are `None` or zero | Model is disconnected from data     | Ensure `ann(x)` influences `obs`                             |\n",
    "| \"Parameter already exists\"   | Re-running cells in Jupyter         | Use `pyro.clear_param_store()`                               |\n",
    "| Dtype mismatch errors        | Mixed float32 / float64 tensors     | Use `torch.set_default_dtype(torch.float64)` and `.double()` |\n",
    "| Learning is unstable or flat | Poor init, too deep network, low LR | Try smaller network, warm-up, check signal-to-noise          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# iii) Record example outputs of model and guide after training\n",
    "\n",
    "prior_predictive_posttrain = predictive(model, num_samples = n_model_samples)()['observations']\n",
    "posterior_posttrain = predictive(guide, num_samples = n_guide_samples)()['alpha']\n",
    "posterior_posttrain_faulty = predictive(guide, num_samples = n_guide_samples)()['is_faulty']\n",
    "posterior_predictive_posttrain = predictive(model, guide = guide, num_samples = n_model_samples)()['observations']\n",
    "\n",
    "\n",
    "# iv) Additional investigations\n",
    "\n",
    "# The model traces do not show anything specific. Apart from random variable \n",
    "# is_faulty and probabilities probs_faulty, everything is as in model_3. The\n",
    "# complicated part of the inference is hidden and perfomed by prepending an \n",
    "# enumeration dimension in front of some samples and enumerating all possible \n",
    "# results.\n",
    "model_trace = pyro.poutine.trace(model).get_trace(T_true, observations = T_meas)\n",
    "guide_trace = pyro.poutine.trace(guide).get_trace(T_true, observations = T_meas)\n",
    "print('These are the shapes inside of the model : \\n {}'.format(model_trace.format_shapes()))\n",
    "print('These are the shapes inside of the guide : \\n {}'.format(guide_trace.format_shapes()))\n",
    "\n",
    "# The parameters of the posterior are again stored in pyro's param_store\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print('Param : {}; Value : {}'.format(name, value))\n",
    "\n",
    "# Estimate the posterior probabilities of each device being faulty. For this,\n",
    "# take the samples of is_faulty from the guide function and average them to see\n",
    "# the probability of is_faulty being assigned to each device.\n",
    "faulty_pretrain = torch.mean(posterior_pretrain_faulty,dim=0)\n",
    "faulty_posttrain = torch.mean(posterior_posttrain_faulty,dim=0)\n",
    "print('is_faulty ground truth : [1, 0, 0, 0, 0]')\n",
    "print('Probabilities of being faulty pre-training : {} \\n'\n",
    "      'Probabilities of being faulty post-training : {} '\n",
    "      .format(faulty_pretrain, faulty_posttrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    4. Interpretations and illustrations\n",
    "\"\"\"\n",
    "\n",
    "# i) Plot and print ELBO loss\n",
    "\n",
    "# The ELBO loss can now only be sampled, so it is noisy.\n",
    "plt.figure(1, dpi = 300)\n",
    "plt.plot(loss_sequence)\n",
    "plt.yscale(\"log\")\n",
    "plt.title('ELBO loss during training (log scale)')\n",
    "plt.xlabel('Epoch nr')\n",
    "plt.ylabel('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ii) Showcase the nonlinear drift\n",
    "\n",
    "# SVI adjusted the parameters in the neural net such that the ELBO was reduced.\n",
    "# We now want to show how the function ann() looks like after training.\n",
    "plt.figure(2, dpi = 300)\n",
    "plt.plot(ann(T_true).detach().T)\n",
    "plt.title('Learned drift function')\n",
    "plt.xlabel('T_true')\n",
    "plt.ylabel('Drift value')\n",
    "\n",
    "# The ELBO loss decreases on average but from epoch to epoch it may jump quite\n",
    "# significantly. A better, less noisy estimation of the ELBO can be done by \n",
    "# increasing the num_particles - that is the number of latent samples used for\n",
    "# ELBO estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iii) Compare model output and data\n",
    "\n",
    "# Create the figure and 2x5 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=False, sharey=False)\n",
    "# Global y-axis limits\n",
    "y_min = T_meas.min()\n",
    "y_max = T_meas.max()\n",
    "\n",
    "# FIRST ROW\n",
    "# First plot: measurement data\n",
    "for i in range(n_device):\n",
    "    axes[0,0].scatter(T_true[i,:], T_meas[i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,0].set_title(\"Measurement data\")\n",
    "axes[0,0].set_xlabel(\"T_true\")\n",
    "axes[0,0].set_ylabel(\"T_meas\")\n",
    "axes[0,0].set_ylim(y_min, y_max)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Second plot: data produced by model pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,1].scatter(T_true[i,:], prior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,1].set_title(\"Data from model pre-training\")\n",
    "axes[0,1].set_xlabel(\"T_true\")\n",
    "axes[0,1].set_ylabel(\"T_meas\")\n",
    "axes[0,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive pre-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[0,2].scatter(T_true[i,:], posterior_predictive_pretrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[0,2].set_title(\"Data from posterior predictive pre-training\")\n",
    "axes[0,2].set_xlabel(\"T_true\")\n",
    "axes[0,2].set_ylabel(\"T_meas\")\n",
    "axes[0,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide pre-training\n",
    "axes[0,3].hist2d(posterior_pretrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_pretrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[0,3].set_title(\"2D Histogram of parameters pre-train\")\n",
    "axes[0,3].set_xlabel(\"alpha_0\")\n",
    "axes[0,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "# SECOND ROW\n",
    "# Second plot: data produced by model post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,1].scatter(T_true[i,:], prior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,1].set_title(\"Data from model post-training\")\n",
    "axes[1,1].set_xlabel(\"T_true\")\n",
    "axes[1,1].set_ylabel(\"T_meas\")\n",
    "axes[1,1].set_ylim(y_min, y_max)\n",
    "\n",
    "# Third plot: data produced by posterior_predictive post-training\n",
    "for k in range(n_model_samples):\n",
    "    for i in range(n_device):\n",
    "        axes[1,2].scatter(T_true[i,:], posterior_predictive_posttrain[k,i,:], s=10, label=f\"T_meas: sensor[{i}]\")\n",
    "axes[1,2].set_title(\"Data from posterior predictive post-training\")\n",
    "axes[1,2].set_xlabel(\"T_true\")\n",
    "axes[1,2].set_ylabel(\"T_meas\")\n",
    "axes[1,2].set_ylim(y_min, y_max)\n",
    "\n",
    "# Fourth plot: data produced by guide post-training\n",
    "axes[1,3].hist2d(posterior_posttrain[:,:,0].flatten().numpy(),\n",
    "                 posterior_posttrain[:,:,1].flatten().numpy(),\n",
    "                 bins=10, cmap='viridis')\n",
    "axes[1,3].set_title(\"2D Histogram of parameters post-train\")\n",
    "axes[1,3].set_xlabel(\"alpha_0\")\n",
    "axes[1,3].set_ylabel(\"alpha_1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó What You‚Äôve Learned\n",
    "\n",
    "You now know how to:\n",
    "\n",
    "‚úÖ Define Bayesian models with **latent continuous and discrete structure**  \n",
    "‚úÖ Add **neural nets** as flexible components without disrupting inference  \n",
    "‚úÖ Perform variational inference using **SVI**, **ELBO**, and **enumeration**  \n",
    "‚úÖ Visualize the learned **posterior distributions**, **model output**, and **latent structure**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
